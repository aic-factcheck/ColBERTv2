{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drchajan/devel/python/FC/aic-nlp-utils/aic_nlp_utils/json.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "/mnt/appl/software/PyTorch/2.0.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "No CUDA runtime is found, using CUDA_HOME='/mnt/appl/software/CUDA/11.7.0'\n",
      "/home/drchajan/devel/python/FC/fc_env_hflarge/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/drchajan/devel/python/FC/fc_env_hflarge/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sqlite3\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from dataclasses import dataclass\n",
    "import datetime as dt\n",
    "from itertools import chain\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import string\n",
    "import pandas as pd\n",
    "import unicodedata as ud\n",
    "from time import time\n",
    "from typing import Dict, Type, Callable, List, Union\n",
    "import sys\n",
    "import ujson\n",
    "\n",
    "from aic_nlp_utils.encoding import nfc\n",
    "from aic_nlp_utils.fever import fever_detokenize\n",
    "from aic_nlp_utils.json import read_jsonl, read_json, write_json, write_jsonl\n",
    "from aic_nlp_utils.wiki import filter_and_fix_wiki_extract_for_lang\n",
    "\n",
    "from sentence_transformers import CrossEncoder, util\n",
    "import textwrap\n",
    "\n",
    "sys.path.insert(0, '/home/drchajan/devel/python/FC/ColBERTv2') # ignore other ColBERT installations\n",
    "\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert import Indexer, Searcher\n",
    "from colbert.data import Queries, Collection\n",
    "from colbert import Trainer\n",
    "from colbert.utilities.prepare_data import import_qacg_split, generate_original_id2pid_mapping, export_as_anserini_collection, anserini_retrieve_claims, sbert_CE_rerank, sbert_BI_rerank, generate_triples_by_retrieval, generate_triples_by_retrieval_nway\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENFever Corpus Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG = \"en\"\n",
    "FEVER_ROOT = \"/mnt/data/factcheck/fever/data-en-lrev\"\n",
    "FEVER_CORPUS = f\"{FEVER_ROOT}/enwiki.jsonl\"\n",
    "FEVER_CORPUS_SQLITE = f\"{FEVER_ROOT}/fever/fever.db\"\n",
    "FEVER_PREDICTIONS = f\"{FEVER_ROOT}/predictions\"\n",
    "COLBERT_ROOT = f\"{FEVER_ROOT}/colbertv2\"\n",
    "\n",
    "# QACG_CLAIMS_ROOT = f\"/mnt/data/factcheck/fever/data-en-lrev/qacg\"\n",
    "\n",
    "TRAIN_DIR = \"fever\" # for original FEVER splits\n",
    "# TRAIN_DIR = \"qacg\" # for QACG generated splits\n",
    "\n",
    "SPLIT_DIR = Path(\"fever-data\")\n",
    "SPLIT_ROOT = Path(FEVER_ROOT, SPLIT_DIR)\n",
    "\n",
    "QUERIES_ROOT = Path(COLBERT_ROOT, \"queries\")\n",
    "TRIPLES_ROOT = Path(COLBERT_ROOT, \"triples\")\n",
    "\n",
    "ANSERINI_ROOT = Path(FEVER_ROOT, \"anserini\")\n",
    "ANSERINI_COLLECTION = str(Path(ANSERINI_ROOT, \"collection\"))\n",
    "ANSERINI_INDEX = str(Path(ANSERINI_ROOT, \"index\"))\n",
    "ANSERINI_RETRIEVED = Path(ANSERINI_ROOT, \"retrieved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON corpus does not match with the SQLITE3 one. Other methods were tested using SQLITE, so use that for experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_corpus(corpus_file):\n",
    "    raw = read_jsonl(corpus_file)\n",
    "    original_ids = set()\n",
    "    corpus = []\n",
    "    for r in tqdm(raw):\n",
    "        # id_ = unicodedata.normalize(\"NFC\", fever_detokenize(r[\"id\"]).strip())\n",
    "        id_ = r[\"id\"]\n",
    "        assert r[\"id\"] == unicodedata.normalize(\"NFC\", r[\"id\"])\n",
    "        if id_ in original_ids: # this happens sometimes due to Wiki snapshot errors...\n",
    "            print(f\"Original ID not unique! {id_}. Skipping...\")\n",
    "            continue\n",
    "        text = unicodedata.normalize(\"NFC\", fever_detokenize(r[\"text\"]).strip())\n",
    "        corpus.append({\"id\": id_, \"text\": text})\n",
    "        original_ids.add(id_)\n",
    "    return corpus\n",
    "\n",
    "# corpus = import_corpus(FEVER_CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_corpus_from_sqlite(corpus_db_file):\n",
    "    original_ids = set()\n",
    "    corpus = []\n",
    "    with sqlite3.connect(corpus_db_file, detect_types=sqlite3.PARSE_DECLTYPES) as connection:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(f\"SELECT id, text FROM documents\")\n",
    "        for id_, text in cursor.fetchall():\n",
    "            if id_ in original_ids: # this happens sometimes due to Wiki snapshot errors...\n",
    "                print(f\"Original ID not unique! {id_}. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            text = fever_detokenize(text)\n",
    "            corpus.append({\"id\": nfc(id_), \"text\": nfc(text)})\n",
    "            original_ids.add(id_)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = import_corpus_from_sqlite(FEVER_CORPUS_SQLITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1928_in_association_football',\n",
       " 'text': 'The following are the football (soccer) events of the year 1928 throughout the world.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents: 5396106 paragraphs: 5396106, paragraphs per document: 1.0\n",
      "paragraph len: min:1, max:323210, mean:506.96684646298644, median:318.0\n"
     ]
    }
   ],
   "source": [
    "def print_stats(corpus):\n",
    "    did_set = set([r[\"id\"] for r in corpus])\n",
    "    ndoc = len(did_set)\n",
    "    npar = len(corpus)\n",
    "    print(f\"documents: {ndoc} paragraphs: {npar}, paragraphs per document: {npar/ndoc}\")\n",
    "    plens = [len(r[\"text\"]) for r in corpus]\n",
    "    print(f\"paragraph len: min:{np.min(plens)}, max:{np.max(plens)}, mean:{np.mean(plens)}, median:{np.median(plens)}\")\n",
    "\n",
    "print_stats(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_id2pid = generate_original_id2pid_mapping(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save corpus as a collection for ColBERT training\n",
    "write_jsonl(Path(COLBERT_ROOT, \"collection.jsonl\"), corpus, mkdir=True)\n",
    "write_json(Path(COLBERT_ROOT, \"original_id2pid.json\"), original_id2pid, mkdir=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSFever Corpus Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG = \"cs\"\n",
    "FEVER_ROOT = \"/mnt/data/factcheck/fever/data-cs-lrev\"\n",
    "FEVER_CORPUS_SQLITE = f\"{FEVER_ROOT}/fever/fever.db\"\n",
    "FEVER_PREDICTIONS = f\"{FEVER_ROOT}/predictions\"\n",
    "COLBERT_ROOT = f\"{FEVER_ROOT}/colbertv2\"\n",
    "\n",
    "SPLIT_DIR = Path(\"fever-data\")\n",
    "SPLIT_ROOT = Path(FEVER_ROOT, SPLIT_DIR)\n",
    "\n",
    "QUERIES_ROOT = Path(COLBERT_ROOT, \"queries\")\n",
    "TRIPLES_ROOT = Path(COLBERT_ROOT, \"triples\")\n",
    "\n",
    "ANSERINI_ROOT = Path(FEVER_ROOT, \"anserini\")\n",
    "ANSERINI_COLLECTION = str(Path(ANSERINI_ROOT, \"collection\"))\n",
    "ANSERINI_INDEX = str(Path(ANSERINI_ROOT, \"index\"))\n",
    "ANSERINI_RETRIEVED = Path(ANSERINI_ROOT, \"retrieved\")\n",
    "\n",
    "#---------------------------------------------\n",
    "\n",
    "# LANG = \"cs\"\n",
    "# FEVER_ROOT = Path(\"/mnt/data/factcheck/fever/data_full_nli-filtered-cs\")\n",
    "# FEVER_DATA = Path(FEVER_ROOT, \"fever-data/F1_titles_anserininew_threshold\")\n",
    "# FEVER_CORPUS_SQLITE = Path(FEVER_ROOT, \"fever/cs_wiki_revid_db_sqlite.db\")\n",
    "\n",
    "# # FEVER_CORPUS = \"/mnt/data/factcheck/fever/data-en-latest/enwiki.jsonl\"\n",
    "# # FEVER_CORPUS_SQLITE = \"/mnt/data/factcheck/fever/data-en-lrev/fever/fever.db\"\n",
    "# # FEVER_ROOT = \"/mnt/data/factcheck/fever/data-en-lrev/fever-data\"\n",
    "# # FEVER_PREDICTIONS = \"/mnt/data/factcheck/fever/data-en-lrev/predictions\"\n",
    "# # COLBERT_ROOT = \"/mnt/data/factcheck/fever/data-en-lrev/colbertv2-jsonl\"\n",
    "# COLBERT_ROOT = Path(FEVER_ROOT, \"colbertv2\")\n",
    "\n",
    "# # \n",
    "# # TRAIN_DIR = \"fever\" # for original FEVER splits\n",
    "# # TRAIN_DIR = \"qacg\" # for QACG generated splits\n",
    "# TRAIN_DIR = \"qacg-r\" # for QACG generated splits based on randomly selected Wiki pages\n",
    "\n",
    "# QACG_CLAIMS_ROOT = Path(FEVER_ROOT, TRAIN_DIR, \"claim\")\n",
    "\n",
    "\n",
    "# HARD_NEGATIVES_ROOT = Path(COLBERT_ROOT, \"hard_negatives\")\n",
    "# ANSERINI_COLLECTION = str(Path(HARD_NEGATIVES_ROOT, \"anserini\", \"collection\"))\n",
    "# ANSERINI_INDEX = str(Path(HARD_NEGATIVES_ROOT, \"anserini\", \"index\"))\n",
    "# HARD_NEGATIVES_RETRIEVED = Path(HARD_NEGATIVES_ROOT, \"retrieved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = import_corpus_from_sqlite(FEVER_CORPUS_SQLITE)\n",
    "original_id2pid = generate_original_id2pid_mapping(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = filter_and_fix_wiki_extract_for_lang(\n",
    "#     Path(FEVER_ROOT, \"wiki-pages\"),\n",
    "#     Path(FEVER_ROOT, \"fever\", \"wiki_extract_filtered_and_fixed_drchajan.jsonl\"), \"cs\", textcol=\"contents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Jean Alesi',\n",
       " 'text': 'Jean Alesi\\n Jean Alesi (narozen jako Giovanni Alesi) (* 11. června 1964, Avignon) je bývalý francouzský pilot Formule 1 italského původu. Jeho kariéra ve Formuli 1 zahrnovala účast v týmech Tyrrell, Benetton F1, Sauber, Prost, Jordan a hlavně Ferrari, kde byl velice oblíbený mezi tifosi. V roce 2006 byl Alesi oceněn Řádem čestné legie (Chevalier de la Legion d ’ honneur.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save corpus as a collection for ColBERT training\n",
    "write_jsonl(Path(COLBERT_ROOT, \"collection.jsonl\"), corpus, mkdir=True)\n",
    "write_json(Path(COLBERT_ROOT, \"original_id2pid.json\"), original_id2pid, mkdir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/453553 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453553/453553 [00:00<00:00, 644907.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Not needed for Sqlite Import\n",
    "class EnFEVER_LREV_ID_Fixer:\n",
    "    '''Our snapshot of EnFEVER most likely does not exactly match the snapshot used by the authors of the FEVER paper. Some of the evidence documents are missing. It seems that most often they have only slightly different names differing in use of underscores (\"_\"). This class tries to match them. \n",
    "    '''\n",
    "    def __init__(self, corpus):\n",
    "        self.fixed_id2original_id = defaultdict(list)\n",
    "        for r in tqdm(corpus):\n",
    "            original_id = unicodedata.normalize(\"NFC\", r[\"id\"])\n",
    "            fixed_id = original_id.replace(\"_\", \"\")\n",
    "            self.fixed_id2original_id[fixed_id].append(original_id)\n",
    "\n",
    "    def fix(self, id_):\n",
    "        fixed_id = unicodedata.normalize(\"NFC\", id_).replace(\"_\", \"\")\n",
    "        if fixed_id not in self.fixed_id2original_id:\n",
    "            return id_\n",
    "        original_ids = self.fixed_id2original_id[fixed_id]\n",
    "        assert len(original_ids) == 1, f\"{id_} => {fixed_id} => {original_ids}\"\n",
    "        return original_ids[0]\n",
    "    \n",
    "enfever_lrev_id_fixer = EnFEVER_LREV_ID_Fixer(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_split(split_file, original_id2pid, fixer=None, new_format=False, ignore_missing_ids=False, bilingual=False):\n",
    "    # use `bilingual` option for splits by Tomas (interleaved CS, EN pages)\n",
    "    raw = read_jsonl(split_file)\n",
    "    data = []\n",
    "    evidence_total = 0\n",
    "    evidence_not_found = 0\n",
    "    no_evidence = 0\n",
    "    for r in tqdm(raw):\n",
    "        if r[\"verifiable\"] == \"VERIFIABLE\":\n",
    "            claim = unicodedata.normalize(\"NFC\", r[\"claim\"])\n",
    "            assert claim == unicodedata.normalize(\"NFC\", claim), \"Claims not NFC! Probably fix it here\"\n",
    "            evidence = set()\n",
    "            for eset in r[\"evidence\"]:\n",
    "                if bilingual:\n",
    "                    assert len(eset) % 2 == 0, eset\n",
    "                    eset = eset[::2]\n",
    "                for e in eset:\n",
    "\n",
    "                    if new_format:\n",
    "                        original_id = e\n",
    "                    else:\n",
    "                        original_id = e[2]\n",
    "                    original_id = unicodedata.normalize(\"NFC\", original_id)\n",
    "                    evidence_total += 1\n",
    "                    if original_id not in original_id2pid:\n",
    "                        if ignore_missing_ids:\n",
    "                            # print(f\"WARNING: missing original_id: {original_id} => REMOVING from evidence!\")\n",
    "                            evidence_not_found += 1\n",
    "                        else:\n",
    "                            assert False, f\"Should not happen with SQLITE3 corpus, comment for JSONL corpus, original_id='{original_id}'\"\n",
    "                            original_id = fixer.fix(original_id) # TODO: check if not using anymore andremove\n",
    "                    else:\n",
    "                        evidence.add(original_id)\n",
    "            id_ = r[\"id\"]\n",
    "            if len(evidence) > 0:\n",
    "                data.append({\"claim\": claim, \"evidence\": list(evidence), \"fever_id\": id_})\n",
    "            else:\n",
    "                # print(f\"WARNING: no evidence for: {r}\")\n",
    "                no_evidence += 1\n",
    "    print(f\"Not found {evidence_not_found}/{evidence_total} evidence documents, {no_evidence} claims had zero evidence\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145449/145449 [00:00<00:00, 495802.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found 0/263822 evidence documents, 0 claims had zero evidence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 641080.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found 0/14475 evidence documents, 0 claims had zero evidence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 608082.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found 0/14150 evidence documents, 0 claims had zero evidence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EnFEVER\n",
    "enfever_lrev_id_fixer = None # SQLITE IMPORT\n",
    "trn_data = import_split(Path(SPLIT_ROOT, \"train.jsonl\"), original_id2pid, fixer=enfever_lrev_id_fixer)\n",
    "dev_data = import_split(Path(SPLIT_ROOT, \"paper_dev.jsonl\"), original_id2pid, fixer=enfever_lrev_id_fixer)\n",
    "tst_data = import_split(Path(SPLIT_ROOT, \"paper_test.jsonl\"), original_id2pid, fixer=enfever_lrev_id_fixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107330/107330 [00:00<00:00, 646736.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found 377/164506 evidence documents, 142 claims had zero evidence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 605641.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found 23/15358 evidence documents, 16 claims had zero evidence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 625934.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found 21/15614 evidence documents, 12 claims had zero evidence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CsFEVER LREV\n",
    "trn_data = import_split(Path(SPLIT_ROOT, \"train_deepl.jsonl\"), original_id2pid, ignore_missing_ids=True)\n",
    "dev_data = import_split(Path(SPLIT_ROOT, \"dev_deepl.jsonl\"), original_id2pid, ignore_missing_ids=True)\n",
    "tst_data = import_split(Path(SPLIT_ROOT, \"test_deepl.jsonl\"), original_id2pid, ignore_missing_ids=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVE? QACG Training Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EnFEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: /mnt/data/factcheck/fever/data-en-lrev/qacg/train_sup_claims.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12549/12549 [00:04<00:00, 2518.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: /mnt/data/factcheck/fever/data-en-lrev/qacg/train_ref_claims.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9807/9807 [00:00<00:00, 36919.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: /mnt/data/factcheck/fever/data-en-lrev/qacg/dev_sup_claims.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:00<00:00, 20199.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: /mnt/data/factcheck/fever/data-en-lrev/qacg/dev_ref_claims.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [00:00<00:00, 42556.32it/s]\n"
     ]
    }
   ],
   "source": [
    "trn_data = import_qacg_split([Path(QACG_CLAIMS_ROOT, \"train_sup_claims.json\"), Path(QACG_CLAIMS_ROOT, \"train_ref_claims.json\")])\n",
    "dev_data = import_qacg_split([Path(QACG_CLAIMS_ROOT, \"dev_sup_claims.json\"), Path(QACG_CLAIMS_ROOT, \"dev_ref_claims.json\")])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CsFEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: /mnt/data/factcheck/fever/data_full_nli-filtered-cs/qacg-r/claim/train_sup_claims-PAV-ner-CNEC_mt5-large-cp59000.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 518/10000 [00:00<00:02, 3266.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN>> claim not NFC, fixing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 21492.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN>> claim not NFC, fixing...\n",
      "WARN>> claim not NFC, fixing...\n",
      "reading: /mnt/data/factcheck/fever/data_full_nli-filtered-cs/qacg-r/claim/train_ref_claims-PAV-ner-CNEC_mt5-large-cp59000.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 648/10000 [00:00<00:03, 2606.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN>> claim not NFC, fixing...\n",
      "WARN>> claim not NFC, fixing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 15174.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: /mnt/data/factcheck/fever/data_full_nli-filtered-cs/qacg-r/claim/dev_sup_claims-PAV-ner-CNEC_mt5-large-cp59000.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 75806.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: /mnt/data/factcheck/fever/data_full_nli-filtered-cs/qacg-r/claim/dev_ref_claims-PAV-ner-CNEC_mt5-large-cp59000.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 83184.01it/s]\n"
     ]
    }
   ],
   "source": [
    "trn_data = import_qacg_split([Path(QACG_CLAIMS_ROOT, \"train_sup_claims-PAV-ner-CNEC_mt5-large-cp59000.json\"), Path(QACG_CLAIMS_ROOT, \"train_ref_claims-PAV-ner-CNEC_mt5-large-cp59000.json\")])\n",
    "dev_data = import_qacg_split([Path(QACG_CLAIMS_ROOT, \"dev_sup_claims-PAV-ner-CNEC_mt5-large-cp59000.json\"), Path(QACG_CLAIMS_ROOT, \"dev_ref_claims-PAV-ner-CNEC_mt5-large-cp59000.json\")])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anserini Hard Negatives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Anserini in the first stage to get hard negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_as_anserini_collection(corpus, ANSERINI_COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-10-25 11:07:44,403 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-10-25 11:07:44,404 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-10-25 11:07:44,404 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-10-25 11:07:44,405 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: /mnt/data/factcheck/fever/data-en-lrev/anserini/collection\n",
      "2023-10-25 11:07:44,405 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-10-25 11:07:44,405 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-10-25 11:07:44,405 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
      "2023-10-25 11:07:44,405 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-10-25 11:07:44,405 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-10-25 11:07:44,406 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-10-25 11:07:44,406 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-10-25 11:07:44,406 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-10-25 11:07:44,406 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-10-25 11:07:44,406 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-10-25 11:07:44,406 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-10-25 11:07:44,407 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-10-25 11:07:44,407 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-10-25 11:07:44,407 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-10-25 11:07:44,407 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: false\n",
      "2023-10-25 11:07:44,407 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: /mnt/data/factcheck/fever/data-en-lrev/anserini/index\n",
      "2023-10-25 11:07:44,409 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-10-25 11:07:44,415 INFO  [main] index.IndexCollection (IndexCollection.java:468) - Using DefaultEnglishAnalyzer\n",
      "2023-10-25 11:07:44,415 INFO  [main] index.IndexCollection (IndexCollection.java:469) - Stemmer: porter\n",
      "2023-10-25 11:07:44,415 INFO  [main] index.IndexCollection (IndexCollection.java:470) - Keep stopwords? false\n",
      "2023-10-25 11:07:44,415 INFO  [main] index.IndexCollection (IndexCollection.java:471) - Stopwords file: null\n",
      "2023-10-25 11:07:44,496 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
      "2023-10-25 11:07:44,496 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in /mnt/data/factcheck/fever/data-en-lrev/anserini/collection\n",
      "2023-10-25 11:07:44,497 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
      "2023-10-25 11:07:44,498 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python -m pyserini.index.lucene \\\n",
    "    -collection JsonCollection \\\n",
    "    -generator DefaultLuceneDocumentGenerator \\\n",
    "    -threads 4 \\\n",
    "    -input {ANSERINI_COLLECTION} \\\n",
    "    -language {LANG} \\\n",
    "    -index {ANSERINI_INDEX} \\\n",
    "    -storePositions -storeDocvectors -storeRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6654/6654 [00:41<00:00, 162.26it/s]\n",
      "100%|██████████| 6650/6650 [00:41<00:00, 161.00it/s]\n",
      "100%|██████████| 71549/71549 [07:23<00:00, 161.20it/s]\n"
     ]
    }
   ],
   "source": [
    "anserini_retrieve_claims(ANSERINI_INDEX, tst_data, 128)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, \"test_anserini.jsonl\"), tst_data, mkdir=True)\n",
    "\n",
    "anserini_retrieve_claims(ANSERINI_INDEX, dev_data, 128)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, \"dev_anserini.jsonl\"), dev_data, mkdir=True)\n",
    "\n",
    "anserini_retrieve_claims(ANSERINI_INDEX, trn_data, 128)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, \"train_anserini.jsonl\"), trn_data, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering k from 71549 to 71502\n",
      "filtering k from 6650 to 6648\n",
      "filtering k from 6654 to 6653\n"
     ]
    }
   ],
   "source": [
    "def ensure_k(data, k):\n",
    "    # keep only claims for which all \"k\" evidence documents were retrieved\n",
    "    olen = len(data)\n",
    "    ndata = list(filter(lambda e: len(e[\"retrieved\"]) >= k, data))\n",
    "    print(f\"filtering k from {olen} to {len(ndata)}\")\n",
    "    return ndata\n",
    "\n",
    "trn_data = ensure_k(trn_data, 128)\n",
    "dev_data = ensure_k(dev_data, 128)\n",
    "tst_data = ensure_k(tst_data, 128)\n",
    "\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, \"train_128_anserini.jsonl\"), trn_data, mkdir=True)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, \"dev_128_anserini.jsonl\"), dev_data, mkdir=True)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, \"test_128_anserini.jsonl\"), tst_data, mkdir=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBERT Reranked Anserini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import splits with hard negatives retrieved by Anserini\n",
    "trn_data_anserini = read_jsonl(Path(HARD_NEGATIVES_RETRIEVED, TRAIN_DIR, \"train_128_anserini.jsonl\"))\n",
    "dev_data_anserini = read_jsonl(Path(HARD_NEGATIVES_RETRIEVED, TRAIN_DIR, \"dev_128_anserini.jsonl\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EnFEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_CE_rerank(dev_data_anserini, corpus, Path(HARD_NEGATIVES_RETRIEVED, TRAIN_DIR, \"dev_128_anserini+minilm.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_CE_rerank(trn_data_anserini, corpus, Path(HARD_NEGATIVES_RETRIEVED, TRAIN_DIR, \"train_128_anserini+minilm.jsonl\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CsFEVER\n",
    "No Czech cross-encoders exist. No luck with bi-encoders. I'll stay with Anserini..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert_BI_rerank(dev_data_anserini[:10], corpus, Path(HARD_NEGATIVES_RETRIEVED, TRAIN_DIR, \"dev_128_anserini+minilm.jsonl\"), model_name=\"deepset/xlm-roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert_BI_rerank(trn_data_anserini, corpus, Path(HARD_NEGATIVES_RETRIEVED, TRAIN_DIR, \"trn_128_anserini+minilm.jsonl\"), model_name=\"deepset/xlm-roberta-base-squad2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EnFEVER MiniLM\n",
    "# trn_data = read_jsonl(Path(HARD_NEGATIVES_RETRIEVED, Path(HARD_NEGATIVES_RETRIEVED, TRAIN_DIR, \"train_128_anserini+minilm.jsonl\")))\n",
    "# dev_data = read_jsonl(Path(HARD_NEGATIVES_RETRIEVED, Path(HARD_NEGATIVES_RETRIEVED, TRAIN_DIR, \"dev_128_anserini+minilm.jsonl\")))\n",
    "\n",
    "# EnFEVER and CsFEVER LREV\n",
    "trn_data = read_jsonl(Path(ANSERINI_RETRIEVED, \"train_128_anserini.jsonl\"))\n",
    "dev_data = read_jsonl(Path(ANSERINI_RETRIEVED, \"dev_128_anserini.jsonl\"))\n",
    "tst_data = read_jsonl(Path(ANSERINI_RETRIEVED, \"test_128_anserini.jsonl\"))\n",
    "\n",
    "# CsFEVER\n",
    "# trn_data = read_jsonl(Path(HARD_NEGATIVES_RETRIEVED, Path(HARD_NEGATIVES_RETRIEVED, TRAIN_DIR, \"train_128_anserini.jsonl\")))\n",
    "# dev_data = read_jsonl(Path(HARD_NEGATIVES_RETRIEVED, Path(HARD_NEGATIVES_RETRIEVED, TRAIN_DIR, \"dev_128_anserini.jsonl\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71502/71502 [00:00<00:00, 1698203.98it/s]\n",
      "100%|██████████| 6648/6648 [00:00<00:00, 1593902.65it/s]\n",
      "100%|██████████| 6653/6653 [00:00<00:00, 1839102.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO move below as for QACG!\n",
    "def export_queries(data, out_file):\n",
    "    queries = []\n",
    "    for r in tqdm(data):\n",
    "        queries.append({\"query\": r[\"claim\"]})\n",
    "    write_jsonl(out_file, queries, mkdir=True)\n",
    "\n",
    "\n",
    "# EnFEVER MiniLM\n",
    "# export_queries(trn_data, Path(COLBERT_ROOT, TRAIN_DIR, \"queries\", \"train_anserini+minilm_queries.jsonl\"))\n",
    "# export_queries(dev_data, Path(COLBERT_ROOT, TRAIN_DIR, \"queries\", \"dev_anserini+minilm_queries.jsonl\"))\n",
    "\n",
    "#EnFEVER and CsFEVER\n",
    "export_queries(trn_data, Path(QUERIES_ROOT, \"train_anserini_queries.jsonl\"))\n",
    "export_queries(dev_data, Path(QUERIES_ROOT, \"dev_anserini_queries.jsonl\"))\n",
    "export_queries(tst_data, Path(QUERIES_ROOT, \"test_anserini_queries.jsonl\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triples by Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_preretrieve = 64\n",
    "# offset = 0\n",
    "# trn_triples = generate_triples_by_retrieval(trn_data, corpus, original_id2pid, n_preretrieve, offset=offset)\n",
    "# dev_triples = generate_triples_by_retrieval(dev_data, corpus, original_id2pid, n_preretrieve, offset=offset)\n",
    "# write_jsonl(Path(COLBERT_ROOT, TRAIN_DIR, \"triples\", f\"train_triples{n_preretrieve}_o{offset}_anserini+minilm.jsonl\"), trn_triples)\n",
    "# write_jsonl(Path(COLBERT_ROOT,TRAIN_DIR,  \"triples\", f\"dev_triples{n_preretrieve}_o{offset}_anserini+minilm.jsonl\"), dev_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 63473,\n",
       "         2: 6340,\n",
       "         3: 895,\n",
       "         4: 361,\n",
       "         5: 181,\n",
       "         7: 87,\n",
       "         6: 84,\n",
       "         8: 34,\n",
       "         9: 19,\n",
       "         10: 10,\n",
       "         11: 6,\n",
       "         12: 3,\n",
       "         13: 3,\n",
       "         15: 2,\n",
       "         14: 2,\n",
       "         17: 1,\n",
       "         21: 1})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([len(e[\"evidence\"]) for e in trn_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109807/109807 [00:13<00:00, 8003.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 109807 triples with 0 failures and 0 random fixes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6666/6666 [00:00<00:00, 7080.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 6666 triples with 0 failures and 0 random fixes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6666/6666 [00:00<00:00, 7051.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 6666 triples with 0 failures and 0 random fixes\n"
     ]
    }
   ],
   "source": [
    "nway = 128\n",
    "\n",
    "# EnFEVER MiniLM\n",
    "# trn_triples = generate_triples_by_retrieval_nway(trn_data, corpus, original_id2pid, nway=nway)\n",
    "# write_jsonl(Path(COLBERT_ROOT, TRAIN_DIR, \"triples\", f\"train_triples_nway{nway}_anserini+minilm.jsonl\"), trn_triples, mkdir=True)\n",
    "\n",
    "# dev_triples = generate_triples_by_retrieval_nway(dev_data, corpus, original_id2pid, nway=nway)\n",
    "# write_jsonl(Path(COLBERT_ROOT, TRAIN_DIR, \"triples\", f\"dev_triples_nway{nway}_anserini+minilm.jsonl\"), dev_triples, mkdir=True)\n",
    "\n",
    "# EnFEVER and CsFEVER\n",
    "trn_triples = generate_triples_by_retrieval_nway(trn_data, corpus, original_id2pid, nway=nway, use_evidence=True)\n",
    "write_jsonl(Path(TRIPLES_ROOT, f\"train_triples_nway{nway}_evidence+anserini.jsonl\"), trn_triples, mkdir=True)\n",
    "\n",
    "dev_triples = generate_triples_by_retrieval_nway(dev_data, corpus, original_id2pid, nway=nway, use_evidence=True)\n",
    "write_jsonl(Path(TRIPLES_ROOT, f\"dev_triples_nway{nway}_evidence+anserini.jsonl\"), dev_triples, mkdir=True)\n",
    "\n",
    "tst_triples = generate_triples_by_retrieval_nway(tst_data, corpus, original_id2pid, nway=nway, use_evidence=True)\n",
    "write_jsonl(Path(TRIPLES_ROOT, f\"test_triples_nway{nway}_evidence+anserini.jsonl\"), tst_triples, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/factcheck/fever/data-cs-lrev/colbertv2/triples')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIPLES_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake Claims\n",
    "Extending actual FEVER claims by claim selected as random sentences from random corpus documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS\n",
    "# language, factor, suffix = \"czech\", 0.0, \"\"\n",
    "# language, factor, suffix = \"czech\", 3.0, \"_factor3\" # this should roughly match QACG number of triples\n",
    "\n",
    "# EN\n",
    "language, factor, suffix = \"english\", 0.0, \"\"\n",
    "language, factor, suffix = \"english\", 2.0, \"_factor2\" # this should roughly match QACG number of triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extending by 219620 fake claims\n",
      "extending by 13332 fake claims\n",
      "extending by 13332 fake claims\n"
     ]
    }
   ],
   "source": [
    "def extend_by_fake_claims(data, corpus, factor=1.0, language=\"czech\", seed=1234):\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    # take random sentences from random documents as claims\n",
    "    rng = np.random.RandomState(seed)\n",
    "    n = int(len(data) * factor)\n",
    "    print(f\"extending by {n} fake claims\")\n",
    "    docs = rng.choice(corpus, n, replace=False)\n",
    "    claims = []\n",
    "    for doc in docs:\n",
    "        claim = str(rng.choice(sent_tokenize(doc[\"text\"], language=language)))\n",
    "        claims.append({\"claim\": claim, \"evidence\": [doc[\"id\"]]})\n",
    "    data += claims\n",
    "    rng.shuffle(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "# CS\n",
    "trn_data = extend_by_fake_claims(trn_data, corpus, factor=factor, language=language, seed=1234)\n",
    "dev_data = extend_by_fake_claims(dev_data, corpus, factor=factor, language=language, seed=1235)\n",
    "tst_data = extend_by_fake_claims(tst_data, corpus, factor=factor, language=language, seed=1236)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'claim': 'Miranda Otto began her film acting career at age 18.',\n",
       "  'evidence': ['Miranda_Otto'],\n",
       "  'fever_id': 23198},\n",
       " {'claim': 'Chirognathus is an extinct genus of conodonts in the family Chirognathidae.',\n",
       "  'evidence': ['Chirognathus']},\n",
       " {'claim': 'Vedam stars only Canadian film actors and actresses.',\n",
       "  'evidence': ['Manoj_Bajpayee',\n",
       "   'Saranya_Ponvannan',\n",
       "   'Anushka_Shetty',\n",
       "   'Allu_Arjun',\n",
       "   'Manchu_Manoj',\n",
       "   'Vedam_-LRB-film-RRB-',\n",
       "   'Lekha_Washington',\n",
       "   'Deeksha_Seth'],\n",
       "  'fever_id': 135962},\n",
       " {'claim': 'Once in the tubule wall, the glucose and amino acids diffuse directly into the blood capillaries along a concentration gradient.',\n",
       "  'evidence': ['Renal_glucose_reabsorption']},\n",
       " {'claim': 'Paramore is a classic rock band.',\n",
       "  'evidence': ['Paramore'],\n",
       "  'fever_id': 96739}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19998/19998 [13:14<00:00, 25.18it/s]\n",
      "100%|██████████| 19998/19998 [12:53<00:00, 25.85it/s]\n",
      "100%|██████████| 329430/329430 [3:37:11<00:00, 25.28it/s]  \n"
     ]
    }
   ],
   "source": [
    "anserini_retrieve_claims(ANSERINI_INDEX, tst_data, 128)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, f\"test_normal+fake{suffix}_anserini.jsonl\"), tst_data, mkdir=True)\n",
    "\n",
    "anserini_retrieve_claims(ANSERINI_INDEX, dev_data, 128)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, f\"dev_normal+fake{suffix}_anserini.jsonl\"), dev_data, mkdir=True)\n",
    "\n",
    "anserini_retrieve_claims(ANSERINI_INDEX, trn_data, 128)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, f\"train_normal+fake{suffix}_anserini.jsonl\"), trn_data, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/factcheck/fever/data-en-lrev/anserini/retrieved')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANSERINI_RETRIEVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering k from 329200 to 329200\n",
      "filtering k from 19980 to 19980\n",
      "filtering k from 19979 to 19979\n"
     ]
    }
   ],
   "source": [
    "def ensure_k(data, k):\n",
    "    # keep only claims for which all \"k\" evidence documents were retrieved\n",
    "    olen = len(data)\n",
    "    ndata = list(filter(lambda e: len(e[\"retrieved\"]) >= k, data))\n",
    "    print(f\"filtering k from {olen} to {len(ndata)}\")\n",
    "    return ndata\n",
    "\n",
    "trn_data = ensure_k(trn_data, 128)\n",
    "dev_data = ensure_k(dev_data, 128)\n",
    "tst_data = ensure_k(tst_data, 128)\n",
    "\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, f\"train_normal+fake{suffix}_128_anserini.jsonl\"), trn_data, mkdir=True)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, f\"dev_normal+fake{suffix}_128_anserini.jsonl\"), dev_data, mkdir=True)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, f\"test_normal+fake{suffix}_128_anserini.jsonl\"), tst_data, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 329200/329200 [00:37<00:00, 8705.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 329200 triples with 0 failures and 0 random fixes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19980/19980 [00:02<00:00, 8467.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 19980 triples with 0 failures and 0 random fixes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19979/19979 [00:02<00:00, 8388.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 19979 triples with 0 failures and 0 random fixes\n"
     ]
    }
   ],
   "source": [
    "trn_data = read_jsonl(Path(ANSERINI_RETRIEVED, f\"train_normal+fake{suffix}_128_anserini.jsonl\"))\n",
    "dev_data = read_jsonl(Path(ANSERINI_RETRIEVED, f\"dev_normal+fake{suffix}_128_anserini.jsonl\"))\n",
    "tst_data = read_jsonl(Path(ANSERINI_RETRIEVED, f\"test_normal+fake{suffix}_128_anserini.jsonl\"))\n",
    "\n",
    "nway = 128\n",
    "# EnFEVER and CsFEVER\n",
    "trn_triples = generate_triples_by_retrieval_nway(trn_data, corpus, original_id2pid, nway=nway, use_evidence=True)\n",
    "write_jsonl(Path(TRIPLES_ROOT, f\"train_triples_normal+fake{suffix}_nway{nway}_evidence+anserini.jsonl\"), trn_triples, mkdir=True)\n",
    "\n",
    "dev_triples = generate_triples_by_retrieval_nway(dev_data, corpus, original_id2pid, nway=nway, use_evidence=True)\n",
    "write_jsonl(Path(TRIPLES_ROOT, f\"dev_triples_normal+fake{suffix}_nway{nway}_evidence+anserini.jsonl\"), dev_triples, mkdir=True)\n",
    "\n",
    "tst_triples = generate_triples_by_retrieval_nway(tst_data, corpus, original_id2pid, nway=nway, use_evidence=True)\n",
    "write_jsonl(Path(TRIPLES_ROOT, f\"test_triples_normal+fake{suffix}_nway{nway}_evidence+anserini.jsonl\"), tst_triples, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/factcheck/fever/data-en-lrev/colbertv2/triples')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIPLES_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 329200/329200 [00:00<00:00, 1513452.26it/s]\n",
      "100%|██████████| 19980/19980 [00:00<00:00, 2021229.44it/s]\n",
      "100%|██████████| 19979/19979 [00:00<00:00, 1726227.74it/s]\n"
     ]
    }
   ],
   "source": [
    "def export_queries(data, out_file):\n",
    "    queries = []\n",
    "    for r in tqdm(data):\n",
    "        queries.append({\"query\": r[\"claim\"]})\n",
    "    write_jsonl(out_file, queries, mkdir=True)\n",
    "\n",
    "#EnFEVER and CsFEVER\n",
    "export_queries(trn_data, Path(QUERIES_ROOT, f\"train_normal+fake{suffix}_queries.jsonl\"))\n",
    "export_queries(dev_data, Path(QUERIES_ROOT, f\"dev_normal+fake{suffix}_queries.jsonl\"))\n",
    "export_queries(tst_data, Path(QUERIES_ROOT, f\"test_normal+fake{suffix}_queries.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/factcheck/fever/data-en-lrev/colbertv2/queries')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERIES_ROOT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_triples_random(data, corpus, original_id2pid, k, seed=1234):\n",
    "    # generate soft negatives by choosing random documents\n",
    "    idx2id = {i: doc[\"id\"] for i, doc in enumerate(corpus)}\n",
    "    id2idx = {doc[\"id\"]: i for i, doc in enumerate(corpus)}\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    def random_docs(posidx: int):\n",
    "        docs = set()\n",
    "        while len(docs) < k:\n",
    "            doc = rng.choice(len(id2idx))\n",
    "            if doc != pos and doc not in docs:\n",
    "                docs.add(doc)\n",
    "        return list(docs)\n",
    "\n",
    "    triples = []\n",
    "    for qid, r in enumerate(tqdm(data)):\n",
    "        for pos in r[\"evidence\"]:\n",
    "            posidx = id2idx[pos]\n",
    "            for neg in random_docs(posidx):\n",
    "                neg = idx2id[neg]\n",
    "                triples.append((qid, original_id2pid[pos], original_id2pid[neg]))\n",
    "    print(f\"generated {len(triples)} triples\")\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310798/310798 [01:29<00:00, 3464.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 9945536 triples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31650/31650 [00:09<00:00, 3480.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 1012800 triples\n"
     ]
    }
   ],
   "source": [
    "k = 32\n",
    "trn_triples = generate_triples_random(trn_data, corpus, original_id2pid, k, seed=1234)\n",
    "dev_triples = generate_triples_random(dev_data, corpus, original_id2pid, k, seed=1235)\n",
    "write_jsonl(Path(COLBERT_ROOT, TRAIN_DIR, \"triples\", f\"train_triples{k}_random.jsonl\"), trn_triples)\n",
    "write_jsonl(Path(COLBERT_ROOT, TRAIN_DIR, \"triples\", f\"dev_triples{k}_random.jsonl\"), dev_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement this, but for full Wikipedia corpora, EnFEVER has only single paragraph=document per page\n",
    "def generate_triples_by_page(data, corpus, original_id2pid, k):\n",
    "    # documents = paragraphs, I use page as term describing whole original text (e.g., Wikipedia page composed of documents=paragraphs)\n",
    "    # takes the positive document and adds k-1 negatives from the same page\n",
    "    id2txt = {doc[\"id\"]: doc[\"text\"] for doc in corpus}\n",
    "    failures = 0\n",
    "    triples = []\n",
    "    # for qid, r in enumerate(data):\n",
    "    #     # those retrieved but not in the annotated evidence will become hard negatives \n",
    "    #     retrieved = set(r[\"retrieved\"][offset:]).difference(r[\"evidence\"])\n",
    "    #     for pos in r[\"evidence\"]:\n",
    "    #         if pos not in id2txt:\n",
    "    #             # may happen for EnFEVER when the snapshot does not exactly match \n",
    "    #             failures += 1\n",
    "    #             continue\n",
    "    #         for neg in list(retrieved)[:k]:\n",
    "    #             triples.append((qid, original_id2pid[pos], original_id2pid[neg]))\n",
    "    # print(f\"generated {len(triples)} triples with {failures} failures\")\n",
    "    return triples\n",
    "\n",
    "# k = 8\n",
    "# dev_triples = generate_triples_by_page(dev_data, corpus, original_id2pid, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Astronomie',\n",
       " 'revid': '21356648',\n",
       " 'url': 'https://cs.wikipedia.org/wiki?curid=10',\n",
       " 'title': 'Astronomie',\n",
       " 'original_id': 10,\n",
       " 'text': 'Astronomie, řecky αστρονομία z άστρον (astron) hvězda a νόμος (nomos) zákon, česky též hvězdářství, je věda, která se zabývá jevy za hranicemi zemské atmosféry. Zvláště tedy výzkumem vesmírných těles, jejich soustav, různých dějů ve vesmíru i vesmírem jako celkem. Historie astronomie. Antika. Astronomie se podobně jako další vědy začala rozvíjet ve starověku. Na území Babylonie však nebylo k popisu používáno již vynalezené geometrie (grafy). První se z astronomie rozvíjela astrometrie, zabývající se měřením poloh hvězd a planet na obloze. Tato oblast astronomie měla velký význam pro navigaci. Podstatnou částí astrometrie je sférická astronomie sloužící k popisu poloh objektů na nebeské sféře, zavádí souřadnice a popisuje významné křivky a body na nebeské sféře. Pojmy ze sférické astronomie se také používají při měření času. Další oblastí astronomie, která se rozvinula, byla nebeská mechanika. Zabývá se pohybem těles v gravitačním poli, například planet ve sluneční soustavě. Základem nebeské mechaniky jsou práce Keplera a Newtona. Aristotelés ve svém díle \"O nebi\" z roku 340 př. n. l. dokázal, že tvar Země musí být kulatý, jelikož stín Země na Měsíci je při zatmění vždy kulatý, což by při plochém tvaru Země nebylo možné. Řekové také zjistili, že pokud sledujeme Polárku z jižnějšího místa na Zemi, jeví se nám níže nad obzorem než pro pozorovatele ze severu, kterému se bude její poloha na obloze jevit výše. Aristotelés dále určil poloměr Země, který ale odhadl na dvojnásobek skutečného poloměru. V aristotelovském modelu Země stojí Měsíc se Sluncem a hvězdami krouží kolem ní, a to po kruhových drahách. Myšlenky Aristotelovy rozvinul ve 2. století našeho letopočtu Klaudios Ptolemaios, který také stavěl Zemi do středu a další objekty nechal obíhat kolem ní ve sférách: Novověk. Roku 1514 navrhl Mikuláš Koperník nový model, ve kterém bylo ve středu soustavy Slunce a planety obíhaly kolem něj po kruhových drahách, setkal se ale s problémy při pozorováních, objekty se nenacházely na správných souřadnicích. Roku 1609 zkonstruoval Galileo Galilei dalekohled, s jehož pomocí objevil čtyři měsíce obíhající kolem planety Jupiter, a tím dokázal Koperníkovu teorii o Slunci ve středu a planetách kroužících kolem. Johannes Kepler zaměnil kruhové dráhy planet za eliptické, čímž bylo dosaženo souladu s pozorovanými polohami těles. V roce 1687 vydal sir Isaac Newton knihu Philosophiae Naturalis Principia Mathematica o poloze těles v prostoru a čase a zákon obecné přitažlivosti, podle něhož jsou k sobě tělesa vázána gravitací, která závisí na hmotnosti těles a na jejich vzdálenosti. Z gravitačního zákona vychází eliptický pohyb planet. Nová doba. Roku 1929 studoval Edwin Hubble daleké galaxie, zjistil rudý posuv, který se zvětšuje se vzdáleností, to byl důkaz o rozpínání vesmíru. Fakt, že se od sebe objekty vzdalují, naznačuje, že někdy v minulosti byly objekty velmi blízko od sebe, tím se zrodily myšlenky o velkém třesku, místě a čase, kdy byl vesmír nekonečně malý a hustý. V letech 1905–1915 napsal Albert Einstein teorii relativity – speciální, ve které zavedl konečnou rychlost světla a obecnou relativitu o gravitaci, čase a prostoru ve velkých rozměrech. Na začátku 20. století vznikla kvantová teorie o chování elementárních částic. Čínská astronomie. Čínská astronomie má velice dlouhou historii a dějepisci považují Číňany za „nejdůslednější a nejpřesnější pozorovatele nebeských jevů na světě před Araby.“ Jména hvězd později rozdělili do 28 kategorií („panství“, [říší]) v dobách dynastie Šang (Shang) v čínské době bronzové a zřejmě se zformovaly za vlády Wu-Tinga (Wu Ding) (1339–1281 př. n. l.). Podrobné záznamy astrologických pozorování započaly v éře válek kolem 4. století př. n. l. a vzkvétaly dále od období dynastie Chan. Čínská astronomie byla rovníková soustředěná na podrobná pozorování hvězd z okolí pólu a byla založená na jiných principech než převládaly v západoevropské astronomii, kde východ a západ slunce zodiakálních souhvězdí tvořil základní ekliptický rámec. Některé prvky indické astronomie se dostaly do Číny při expanzi buddhismu po dynastii Chan (25–220 n.l.), ale nejpodrobnější vtělení indické astronomie nastalo za dynastie Tchang (Tang, 618–907), kdy mnozí indičtí astronomové přesídlili do čínského hlavního města a čínští učenci, jako velký tantrický buddhistický mnich a matematik Ji-Šing (Yi Xing) propracoval její systém. Astronomie islámského středověku úzce spolupracovala se svými čínskými kolegy během dynastie Juan (Yuan) a po období poměrného ústupu za dynastie Ming astronomie ožila podněty západní kosmologie a techniky po vzniku jezuitských misií. Dalekohled byl zaveden v 17. století. V roce 1669 byla pekingská observatoř přestavěna pod vedením Ferdinanda Verbiesta. Dnešní Čína pokračuje v astronomických aktivitách s mnoha hvězdárnami a vlastním vesmírným programem. Části astronomie. Od novověku do současnosti se astronomie nesmírně rozšířila a vznikla celá řada nových oblastí výzkumu, které lze velmi zhruba rozdělit na pozorování a teorii, nebo podle objektu zájmu. Astronomické pozorování. Astronom, česky hvězdář, se zabývá zkoumáním vesmíru. Kromě profesionálních astronomů se astronomii věnuje i řada astronomů amatérských. Nejvýznamnějším zdrojem informací o vesmíru je elektromagnetické záření. Část jeho vlnových délek, vnímatelná očima, je světlo. Obory astronomického pozorování podle využívaných vlnových délek jsou Nejstarší a nejdůležitější je optická astronomie, využívající světlo. Rozvoj dalších oborů souvisel s vývojem techniky. Například radioastronomie se začala rozvíjet ve 30. letech 20. století, kdy Karl Guthe Jansky při zkoumání zdrojů šumu rušících rádiové hovory objevil rádiové emise centra naší Galaxie. Atmosféra Země mnoho vlnových délek účinně pohlcuje, takže gama a rentgenové pozorování se mohlo konat jen pomocí stratosférických balónů a výrazný rozvoj se dostavil teprve s pokrokem kosmonautiky. Ještě exotičtější je pozorování jiných částic než elektromagnetického záření. Hypotetická gravitační astronomie by měla pozorovat gravitační vlny. V současnosti jsou převažujícím způsobem detekce velké interferometry. První pozorování gravitačních vln proběhlo 14. září 2015 na americkém detektoru LIGO, i když nepřímé důkazy byly předloženy již dříve. Astronomická teorie. Obecným teoretickým oborem je astrofyzika. Zabývá se fyzikou hvězd a mezihvězdné hmoty (hustotou, teplotou, chemickým složením atd.). Kosmologie studuje vesmír jako celek a zvláště jeho vznik, současný a budoucí vývoj. Astrobiologie se zabývá možnostmi existence života ve vesmíru. Vztah astronomie k dalším vědám. Astronomie má nejužší vztah s fyzikou. Astronomická teorie je v podstatě fyzika astronomických systémů. Naopak astronomické systémy jsou pro velkou část fyzikální teorie nejdůležitější „laboratoří“, přirozeně především ve velkých prostorových a časových měřítkách se projevuje gravitace a testuje obecná teorie relativity. Ve vesmíru se vyskytují i extrémní podmínky, které nejsou zatím dosažitelné v laboratořích, například tlak, hustota, teplota, magnetické pole a další. Významný vztah má astronomie i k religionistice.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Snoop_Dogg', 6912),\n",
       " ('Marlon_Brando', 6880),\n",
       " ('United_States', 6112),\n",
       " ('Wyatt_Earp', 5888),\n",
       " ('Michael_Jackson', 5856),\n",
       " ('United_Kingdom', 5760),\n",
       " ('Adele', 5152),\n",
       " ('Miley_Cyrus', 5024),\n",
       " ('Tim_Rice', 4832),\n",
       " ('The_Beatles', 4544),\n",
       " ('International_relations', 4480),\n",
       " ('A_Song_of_Ice_and_Fire', 4480),\n",
       " ('Abraham_Lincoln', 4448),\n",
       " ('David_Beckham', 4320),\n",
       " ('Anne_Hathaway', 4192),\n",
       " ('One_Direction', 4192),\n",
       " ('Frank_Sinatra', 4160),\n",
       " ('Oliver_Reed', 4096),\n",
       " ('Deadpool_-LRB-_film_-RRB-', 4064),\n",
       " ('Bradley_Cooper', 4064)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter(pid2original_id[t[1]] for t in trn_triples)\n",
    "c.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(Path(COLBERT_ROOT, \"triples\", f\"train_triples{n_preretrieve}_o{offset}.jsonl\"), trn_triples)\n",
    "write_jsonl(Path(COLBERT_ROOT, \"triples\", f\"dev_triples{n_preretrieve}_o{offset}.jsonl\"), dev_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_triples = read_jsonl(Path(COLBERT_ROOT, f\"dev_triples1_random.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telemundo is a English-language television network.\n",
      "Hispanic_and_Latino_Americans\n",
      "\n",
      "Cnaphalocrocis_poeyalis\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "for qid, pos, neg in dev_triples[1:]:\n",
    "    claim = dev_data[qid]['claim']\n",
    "    # pos = textwrap.fill(corpus[pos]['text'])\n",
    "    # neg = textwrap.fill(corpus[neg]['text'])\n",
    "    pos = corpus[pos][\"id\"]\n",
    "    neg = corpus[neg][\"id\"]\n",
    "\n",
    "    print(f\"{claim}\")\n",
    "    print(f\"{pos}\")\n",
    "    print()\n",
    "    print(f\"{neg}\")\n",
    "    print(\"-----------\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample QACG-(CS, EN) to (Cs, En)FEVER Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target sizes: Counter({'s': 53542, 'n': 35639, 'r': 18149}), total: 107330\n",
      " final sizes: Counter({'s': 53542, 'n': 35639, 'r': 18149}), total: 107330\n",
      "target sizes: Counter({'n': 3333, 's': 3333, 'r': 3333}), total: 9999\n",
      " final sizes: Counter({'n': 3333, 'r': 3333, 's': 3333}), total: 9999\n",
      "target sizes: Counter({'s': 3333, 'n': 3333, 'r': 3333}), total: 9999\n",
      " final sizes: Counter({'s': 3333, 'r': 3333, 'n': 3333}), total: 9999\n"
     ]
    }
   ],
   "source": [
    "def sample_qacg_to_fever(src_split, size_split, dst_split, seed=1234):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    src_data = read_jsonl(src_split)\n",
    "    size_data = read_jsonl(size_split)\n",
    "    rename_labels = {\"SUPPORTS\": \"s\", \"REFUTES\": \"r\", \"NOT ENOUGH INFO\": \"n\"}\n",
    "    sizes = Counter(rename_labels[r[\"label\"]] for r in size_data)\n",
    "    print(f\"target sizes: {sizes}, total: {np.sum(list(sizes.values()))}\")\n",
    "    dst_data = []\n",
    "    for label in sizes.keys():\n",
    "        samples = [r for r in src_data if r[\"label\"] == label]\n",
    "        # print(label, samples)\n",
    "        dst_data += list(rng.choice(samples, sizes[label], replace=False))\n",
    "    rng.shuffle(dst_data)\n",
    "    sizes = Counter(r[\"label\"] for r in dst_data)\n",
    "    print(f\" final sizes: {sizes}, total: {np.sum(list(sizes.values()))}\")\n",
    "    write_jsonl(dst_split, dst_data)\n",
    "\n",
    "\n",
    "# sample_qacg_to_fever(src_split=\"/mnt/data/factcheck/wiki/cs/20230801/qacg/splits/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/train_balanced.jsonl\",\n",
    "#                      size_split=\"/mnt/data/factcheck/fever/data-cs-lrev/fever-data/train_deepl.jsonl\",\n",
    "#                      dst_split=\"/mnt/data/factcheck/wiki/cs/20230801/qacg/splits/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/train_fever_size.jsonl\"\n",
    "#                      )\n",
    "\n",
    "# sample_qacg_to_fever(src_split=\"/mnt/data/factcheck/wiki/cs/20230801/qacg/splits/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/dev_balanced.jsonl\",\n",
    "#                      size_split=\"/mnt/data/factcheck/fever/data-cs-lrev/fever-data/dev_deepl.jsonl\",\n",
    "#                      dst_split=\"/mnt/data/factcheck/wiki/cs/20230801/qacg/splits/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/dev_fever_size.jsonl\"\n",
    "#                      )\n",
    "# sample_qacg_to_fever(src_split=\"/mnt/data/factcheck/wiki/cs/20230801/qacg/splits/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/test_balanced.jsonl\",\n",
    "#                      size_split=\"/mnt/data/factcheck/fever/data-cs-lrev/fever-data/test_deepl.jsonl\",\n",
    "#                      dst_split=\"/mnt/data/factcheck/wiki/cs/20230801/qacg/splits/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/test_fever_size.jsonl\"\n",
    "#                      )\n",
    "\n",
    "# sample_qacg_to_fever(src_split=\"/mnt/data/factcheck/wiki/en/20230801/qacg/splits/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k/train_balanced.jsonl\",\n",
    "#                      size_split=\"/mnt/data/factcheck/fever/data-en-lrev/fever-data/train.jsonl\",\n",
    "#                      dst_split=\"/mnt/data/factcheck/wiki/en/20230801/qacg/splits/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k/train_fever_size.jsonl\"\n",
    "#                      )\n",
    "\n",
    "# sample_qacg_to_fever(src_split=\"/mnt/data/factcheck/wiki/en/20230801/qacg/splits/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k/dev_balanced.jsonl\",\n",
    "#                      size_split=\"/mnt/data/factcheck/fever/data-en-lrev/fever-data/paper_dev.jsonl\",\n",
    "#                      dst_split=\"/mnt/data/factcheck/wiki/en/20230801/qacg/splits/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k/dev_fever_size.jsonl\"\n",
    "#                      )\n",
    "# sample_qacg_to_fever(src_split=\"/mnt/data/factcheck/wiki/en/20230801/qacg/splits/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k/test_balanced.jsonl\",\n",
    "#                      size_split=\"/mnt/data/factcheck/fever/data-en-lrev/fever-data/paper_test.jsonl\",\n",
    "#                      dst_split=\"/mnt/data/factcheck/wiki/en/20230801/qacg/splits/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k/test_fever_size.jsonl\"\n",
    "#                      )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine FEVER and QACG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(src_queries_lsts, src_triples_lsts, dst_queries, dst_triples, seed=1234):\n",
    "    assert len(src_queries_lsts) == len(src_triples_lsts)\n",
    "\n",
    "    queries = []\n",
    "    triples = []\n",
    "    offset = 0\n",
    "    for src_queries, src_triples in zip(src_queries_lsts, src_triples_lsts):\n",
    "        Q = read_jsonl(src_queries)\n",
    "        T = read_jsonl(src_triples)\n",
    "        assert len(Q) == len(T)\n",
    "        queries += Q\n",
    "        for t in T:\n",
    "            t[0] += offset\n",
    "            triples.append(t)\n",
    "        offset += len(Q)\n",
    "        \n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    write_jsonl(dst_queries, queries, mkdir=True)\n",
    "    write_jsonl(dst_triples, triples, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EnFEVER\n",
    "combine_data(\n",
    "    src_queries_lsts=[\n",
    "        Path(COLBERT_ROOT, \"fever\", \"queries\", \"train_anserini+minilm_queries.jsonl\"),\n",
    "        Path(COLBERT_ROOT, \"qacg\", \"queries\", \"train_anserini+minilm_queries.jsonl\")],\n",
    "    src_triples_lsts=[\n",
    "        Path(COLBERT_ROOT, \"fever\", \"triples\", \"train_triples_nway128_anserini+minilm.jsonl\"),\n",
    "        Path(COLBERT_ROOT, \"qacg\", \"triples\", \"train_triples_nway128_anserini+minilm.jsonl\")],\n",
    "    dst_queries=Path(COLBERT_ROOT, \"fever+qacg\", \"queries\", \"train_anserini+minilm_queries.jsonl\"),\n",
    "    dst_triples=Path(COLBERT_ROOT, \"fever+qacg\", \"triples\", \"train_triples_nway128_anserini+minilm.jsonl\")\n",
    ")\n",
    "\n",
    "combine_data(\n",
    "    src_queries_lsts=[\n",
    "        Path(COLBERT_ROOT, \"fever\", \"queries\", \"dev_anserini+minilm_queries.jsonl\"),\n",
    "        Path(COLBERT_ROOT, \"qacg\", \"queries\", \"dev_anserini+minilm_queries.jsonl\")],\n",
    "    src_triples_lsts=[\n",
    "        Path(COLBERT_ROOT, \"fever\", \"triples\", \"dev_triples_nway128_anserini+minilm.jsonl\"),\n",
    "        Path(COLBERT_ROOT, \"qacg\", \"triples\", \"dev_triples_nway128_anserini+minilm.jsonl\")],\n",
    "    dst_queries=Path(COLBERT_ROOT, \"fever+qacg\", \"queries\", \"dev_anserini+minilm_queries.jsonl\"),\n",
    "    dst_triples=Path(COLBERT_ROOT, \"fever+qacg\", \"triples\", \"dev_triples_nway128_anserini+minilm.jsonl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CsFEVER\n",
    "combine_data(\n",
    "    src_queries_lsts=[\n",
    "        Path(COLBERT_ROOT, \"fever\", \"queries\", \"train_anserini_queries.jsonl\"),\n",
    "        Path(COLBERT_ROOT, \"qacg\", \"queries\", \"train_anserini_queries.jsonl\")],\n",
    "    src_triples_lsts=[\n",
    "        Path(COLBERT_ROOT, \"fever\", \"triples\", \"train_triples_nway128_anserini.jsonl\"),\n",
    "        Path(COLBERT_ROOT, \"qacg\", \"triples\", \"train_triples_nway128_anserini.jsonl\")],\n",
    "    dst_queries=Path(COLBERT_ROOT, \"fever+qacg\", \"queries\", \"train_anserini_queries.jsonl\"),\n",
    "    dst_triples=Path(COLBERT_ROOT, \"fever+qacg\", \"triples\", \"train_triples_nway128_anserini.jsonl\")\n",
    ")\n",
    "\n",
    "combine_data(\n",
    "    src_queries_lsts=[\n",
    "        Path(COLBERT_ROOT, \"fever\", \"queries\", \"dev_anserini_queries.jsonl\"),\n",
    "        Path(COLBERT_ROOT, \"qacg\", \"queries\", \"dev_anserini_queries.jsonl\")],\n",
    "    src_triples_lsts=[\n",
    "        Path(COLBERT_ROOT, \"fever\", \"triples\", \"dev_triples_nway128_anserini.jsonl\"),\n",
    "        Path(COLBERT_ROOT, \"qacg\", \"triples\", \"dev_triples_nway128_anserini.jsonl\")],\n",
    "    dst_queries=Path(COLBERT_ROOT, \"fever+qacg\", \"queries\", \"dev_anserini_queries.jsonl\"),\n",
    "    dst_triples=Path(COLBERT_ROOT, \"fever+qacg\", \"triples\", \"dev_triples_nway128_anserini.jsonl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hflarge",
   "language": "python",
   "name": "hflarge"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
