{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drchajan/devel/python/FC/aic-nlp-utils/aic_nlp_utils/json.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sqlite3\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from dataclasses import dataclass\n",
    "import datetime as dt\n",
    "from itertools import chain\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import string\n",
    "import pandas as pd\n",
    "import unicodedata as ud\n",
    "from time import time\n",
    "from typing import Dict, Type, Callable, List, Union\n",
    "import sys\n",
    "import ujson\n",
    "\n",
    "from aic_nlp_utils.json import read_jsonl, read_json, write_json, write_jsonl\n",
    "from aic_nlp_utils.encoding import nfc\n",
    "from aic_nlp_utils.fever import fever_detokenize\n",
    "from sentence_transformers import CrossEncoder, util\n",
    "import textwrap\n",
    "\n",
    "sys.path.insert(0, '/home/drchajan/devel/python/FC/ColBERTv2') # ignore other ColBERT installations\n",
    "\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert import Indexer, Searcher\n",
    "from colbert.data import Queries, Collection\n",
    "from colbert import Trainer\n",
    "from colbert.utilities.prepare_data import import_qacg_split, import_qacg_split_subsample, generate_original_id2pid_mapping, export_as_anserini_collection, anserini_retrieve_claims, sbert_CE_rerank, generate_triples_by_retrieval, generate_triples_by_retrieval_nway\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/factcheck/wiki/en/20240201/qacg/splits/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# APPROACH = \"full\" # all generated data\n",
    "# APPROACH = \"balanced\" # balanced classes\n",
    "APPROACH = \"balanced_shuf\" # balanced classes, shuffled\n",
    "# APPROACH = \"fever_size\" # subsampled to have exact fever distribution\n",
    "\n",
    "# Anserini fails for Polish so use the default English config\n",
    "# LANG, NER_DIR, ANSERINI_LANG = \"cs\", \"PAV-ner-CNEC\", \"cs\"\n",
    "LANG, NER_DIR, ANSERINI_LANG = \"en\", \"stanza\", \"en\"\n",
    "# LANG, NER_DIR, ANSERINI_LANG = \"sk\", \"crabz_slovakbert-ner\", \"sk\"\n",
    "# LANG, NER_DIR, ANSERINI_LANG = \"pl\", \"stanza\", \"en\"\n",
    "\n",
    "# DATE = \"20230220\"\n",
    "# DATE = \"20230801\"\n",
    "DATE = \"20240201\" # CEDMO\n",
    "\n",
    "WIKI_ROOT = f\"/mnt/data/factcheck/wiki/{LANG}/{DATE}\"\n",
    "WIKI_CORPUS = f\"{WIKI_ROOT}/paragraphs/{LANG}wiki-{DATE}-paragraphs.jsonl\"\n",
    "WIKI_LINENO2ID = f\"{WIKI_ROOT}/paragraphs/{LANG}wiki-{DATE}-paragraphs_lineno2id.json\"\n",
    "# WIKI_PREDICTIONS = f\"{WIKI_ROOT}/predictions\"\n",
    "\n",
    "QACG_ROOT = f\"{WIKI_ROOT}/qacg\"\n",
    "\n",
    "QG_DIR = \"mt5-large_all-cp126k\"\n",
    "QACG_DIR = \"mt5-large_all-cp156k\"\n",
    "\n",
    "SPLIT_DIR = Path(\"splits\", NER_DIR, QG_DIR, QACG_DIR)\n",
    "SPLIT_ROOT = Path(QACG_ROOT, SPLIT_DIR)\n",
    "\n",
    "CLAIM_DIR = Path(\"claim\", NER_DIR, QG_DIR, QACG_DIR)\n",
    "CLAIM_ROOT = Path(QACG_ROOT, CLAIM_DIR)\n",
    "\n",
    "\n",
    "TRAIN_FILES = {\n",
    "    \"s\": Path(CLAIM_ROOT, \"train_support.json\"), \n",
    "    \"r\": Path(CLAIM_ROOT, \"train_refute.json\"),\n",
    "    # \"n\": Path(CLAIM_ROOT, \"train_nei.json\")\n",
    "    }\n",
    "\n",
    "DEV_FILES = {\n",
    "    \"s\": Path(CLAIM_ROOT, \"dev_support.json\"), \n",
    "    \"r\": Path(CLAIM_ROOT, \"dev_refute.json\"),\n",
    "    # \"n\": Path(CLAIM_ROOT, \"dev_nei.json\")\n",
    "    }\n",
    "\n",
    "TEST_FILES = {\n",
    "    \"s\": Path(CLAIM_ROOT, \"test_support.json\"), \n",
    "    \"r\": Path(CLAIM_ROOT, \"test_refute.json\"),\n",
    "    # \"n\": Path(CLAIM_ROOT, \"test_nei.json\")\n",
    "    }\n",
    "\n",
    "COLBERT_ROOT = f\"/mnt/data/factcheck/wiki/{LANG}/{DATE}/colbertv2/qacg\"\n",
    "\n",
    "QUERIES_ROOT = Path(COLBERT_ROOT, \"queries\", NER_DIR, QG_DIR, QACG_DIR)\n",
    "TRIPLES_ROOT = Path(COLBERT_ROOT, \"triples\", NER_DIR, QG_DIR, QACG_DIR)\n",
    "\n",
    "ANSERINI_ROOT = Path(WIKI_ROOT, \"anserini\")\n",
    "ANSERINI_COLLECTION = str(Path(ANSERINI_ROOT, \"collection\"))\n",
    "ANSERINI_INDEX = str(Path(ANSERINI_ROOT, \"index\"))\n",
    "ANSERINI_RETRIEVED = Path(ANSERINI_ROOT, \"retrieved\", NER_DIR, QG_DIR, QACG_DIR)\n",
    "SPLIT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_corpus(corpus_file):\n",
    "    # it already has correct format\n",
    "    raw = read_jsonl(corpus_file, show_progress=True)\n",
    "    for e in raw:\n",
    "        e[\"id\"] = nfc(e[\"id\"])\n",
    "        e[\"did\"] = nfc(e[\"did\"])\n",
    "        e[\"text\"] = nfc(e[\"text\"])\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008988380432128906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a009076dcd461fb53e227f7a981aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'Anarchism_2',\n",
       " 'did': 'Anarchism',\n",
       " 'bid': 2,\n",
       " 'text': 'Anarchism\\n\\nAnarchists employ diverse approaches, which may be generally divided into revolutionary and evolutionary strategies; there is significant overlap between the two. Evolutionary methods try to simulate what an anarchist society might be like, but revolutionary tactics, which have historically taken a violent turn, aim to overthrow authority and the state. Many facets of human civilization have been influenced by anarchist theory, critique, and praxis.\\n\\nAnarchism\\n\\nEtymology, terminology, and definition.\\n\\nThe etymological origin of \"anarchism\" is from the Ancient Greek \"anarkhia\", meaning \"without a ruler\", composed of the prefix \"an-\" (\"without\") and the word \"arkhos\" (\"leader\" or \"ruler\"). The suffix \"-ism\" denotes the ideological current that favours anarchy. \"Anarchism\" appears in English from 1642 as \"anarchisme\" and \"anarchy\" from 1539; early English usages emphasised a sense of disorder. Various factions within the French Revolution labelled their opponents as \"anarchists\", although few such accused shared many views with later anarchists. Many revolutionaries of the 19th century such as William Godwin (1756–1836) and Wilhelm Weitling (1808–1871) would contribute to the anarchist doctrines of the next generation but did not use \"anarchist\" or \"anarchism\" in describing themselves or their beliefs.',\n",
       " 'url': 'https://en.wikipedia.org/wiki?curid=12',\n",
       " 'revid': '244263'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = import_corpus(WIKI_CORPUS)\n",
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents: 6206719 paragraphs: 15689008, paragraphs per document: 2.527745818684558\n",
      "paragraph len: min:100, max:116665, mean:1084.785491982667, median:1142.0\n"
     ]
    }
   ],
   "source": [
    "def print_stats(corpus):\n",
    "    did_set = set([r[\"did\"] for r in corpus])\n",
    "    ndoc = len(did_set)\n",
    "    npar = len(corpus)\n",
    "    print(f\"documents: {ndoc} paragraphs: {npar}, paragraphs per document: {npar/ndoc}\")\n",
    "    plens = [len(r[\"text\"]) for r in corpus]\n",
    "    print(f\"paragraph len: min:{np.min(plens)}, max:{np.max(plens)}, mean:{np.mean(plens)}, median:{np.median(plens)}\")\n",
    "\n",
    "print_stats(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_id2pid = generate_original_id2pid_mapping(corpus)\n",
    "lineno2id = {i: r[\"id\"] for i, r in enumerate(corpus)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN this just for the first time\n",
    "write_json(Path(COLBERT_ROOT, \"original_id2pid.json\"), original_id2pid, mkdir=True)\n",
    "write_jsonl(Path(COLBERT_ROOT, \"collection.jsonl\"), corpus, mkdir=True)\n",
    "write_json(WIKI_LINENO2ID, lineno2id, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: /mnt/data/factcheck/wiki/pl/20230801/qacg/claim/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k/train_support.json\n",
      "WARN>> claim not NFC, fixing...\n",
      "WARN>> claim not NFC, fixing...\n",
      "WARN>> claim not NFC, fixing...\n",
      "WARN>> claim not NFC, fixing...\n",
      "reading: /mnt/data/factcheck/wiki/pl/20230801/qacg/claim/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k/train_refute.json\n",
      "reading: /mnt/data/factcheck/wiki/pl/20230801/qacg/claim/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k/dev_support.json\n",
      "reading: /mnt/data/factcheck/wiki/pl/20230801/qacg/claim/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k/dev_refute.json\n",
      "reading: /mnt/data/factcheck/wiki/pl/20230801/qacg/claim/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k/test_support.json\n",
      "reading: /mnt/data/factcheck/wiki/pl/20230801/qacg/claim/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k/test_refute.json\n"
     ]
    }
   ],
   "source": [
    "if APPROACH == \"full\":\n",
    "    trn_data = import_qacg_split(TRAIN_FILES)\n",
    "    dev_data = import_qacg_split(DEV_FILES)\n",
    "    tst_data = import_qacg_split(TEST_FILES)\n",
    "elif APPROACH in [\"balanced\", \"balanced_shuf\"]:\n",
    "    trn_data = import_qacg_split_subsample(TRAIN_FILES, subsample=98403, seed=1234)\n",
    "    dev_data = import_qacg_split_subsample(DEV_FILES, subsample=10029, seed=1234)\n",
    "    tst_data = import_qacg_split_subsample(TEST_FILES, subsample=9480, seed=1234)\n",
    "    if APPROACH == \"balanced_shuf\":\n",
    "        rng = np.random.RandomState(1234)\n",
    "        rng.shuffle(trn_data)\n",
    "        rng.shuffle(dev_data)\n",
    "        rng.shuffle(tst_data)\n",
    "elif APPROACH == \"fever_size\":\n",
    "    print(\"Created in prepare_data_fever.ipynb\")\n",
    "    trn_data = read_jsonl(Path(SPLIT_ROOT, \"train_fever_size.jsonl\"))\n",
    "    dev_data = read_jsonl(Path(SPLIT_ROOT, \"dev_fever_size.jsonl\"))\n",
    "    tst_data = read_jsonl(Path(SPLIT_ROOT, \"test_fever_size.jsonl\"))\n",
    "else:\n",
    "    assert False, APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original claims: 196806, unique: 183204\n",
      "original claims: 20058, unique: 18685\n",
      "original claims: 18960, unique: 17731\n"
     ]
    }
   ],
   "source": [
    "def unique_claims(data):\n",
    "    claims = set()\n",
    "    new_data = []\n",
    "    for e in data:\n",
    "        if e[\"claim\"] not in claims:\n",
    "            new_data.append(e)\n",
    "            claims.add(e[\"claim\"])\n",
    "    print(f\"original claims: {len(data)}, unique: {len(new_data)}\")\n",
    "    return new_data\n",
    "\n",
    "trn_data = unique_claims(trn_data)\n",
    "dev_data = unique_claims(dev_data)\n",
    "tst_data = unique_claims(tst_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/factcheck/wiki/pl/20230801/qacg/splits/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_jsonl(Path(SPLIT_ROOT, f\"train_{APPROACH}.jsonl\"), trn_data, mkdir=True)\n",
    "write_jsonl(Path(SPLIT_ROOT, f\"dev_{APPROACH}.jsonl\"), dev_data, mkdir=True)\n",
    "write_jsonl(Path(SPLIT_ROOT, f\"test_{APPROACH}.jsonl\"), tst_data, mkdir=True)\n",
    "SPLIT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183204"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'claim': 'Iwan Własow został zamordowany w obozie dla jeńców wojennych.',\n",
       "  'label': 's',\n",
       "  'evidence': ['Iwan_Własow_(dyplomata)_1']},\n",
       " {'claim': 'W drugiej rundzie US Open wygrała z Eugenie Bouchard.',\n",
       "  'label': 's',\n",
       "  'evidence': ['Angelique_Kerber_5']})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_data[0], trn_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183204/183204 [00:00<00:00, 1511458.23it/s]\n",
      "100%|██████████| 18685/18685 [00:00<00:00, 1626451.60it/s]\n",
      "100%|██████████| 17731/17731 [00:00<00:00, 1766993.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/factcheck/wiki/pl/20230801/colbertv2/qacg/queries/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def export_queries(data, out_file):\n",
    "    queries = []\n",
    "    for r in tqdm(data):\n",
    "        queries.append({\"query\": r[\"claim\"]})\n",
    "    write_jsonl(out_file, queries, mkdir=True)\n",
    "\n",
    "export_queries(trn_data, Path(QUERIES_ROOT, f\"train_qacg_queries_{APPROACH}.jsonl\"))\n",
    "export_queries(dev_data, Path(QUERIES_ROOT, f\"dev_qacg_queries_{APPROACH}.jsonl\"))\n",
    "export_queries(tst_data, Path(QUERIES_ROOT, f\"test_qacg_queries_{APPROACH}.jsonl\"))\n",
    "QUERIES_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anserini Hard Negatives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Anserini in the first stage to get hard negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_as_anserini_collection(corpus, ANSERINI_COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-09-13 19:16:56,787 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-09-13 19:16:56,788 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-09-13 19:16:56,788 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-09-13 19:16:56,788 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: /mnt/data/factcheck/wiki/pl/20230801/anserini/collection\n",
      "2023-09-13 19:16:56,789 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-09-13 19:16:56,789 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-09-13 19:16:56,789 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
      "2023-09-13 19:16:56,789 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-09-13 19:16:56,789 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-09-13 19:16:56,789 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-09-13 19:16:56,789 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-09-13 19:16:56,790 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-09-13 19:16:56,790 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-09-13 19:16:56,790 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-09-13 19:16:56,790 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-09-13 19:16:56,790 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-09-13 19:16:56,791 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-09-13 19:16:56,791 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-09-13 19:16:56,791 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: false\n",
      "2023-09-13 19:16:56,791 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: /mnt/data/factcheck/wiki/pl/20230801/anserini/index\n",
      "2023-09-13 19:16:56,793 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-09-13 19:16:56,799 INFO  [main] index.IndexCollection (IndexCollection.java:468) - Using DefaultEnglishAnalyzer\n",
      "2023-09-13 19:16:56,799 INFO  [main] index.IndexCollection (IndexCollection.java:469) - Stemmer: porter\n",
      "2023-09-13 19:16:56,799 INFO  [main] index.IndexCollection (IndexCollection.java:470) - Keep stopwords? false\n",
      "2023-09-13 19:16:56,799 INFO  [main] index.IndexCollection (IndexCollection.java:471) - Stopwords file: null\n",
      "2023-09-13 19:16:56,866 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
      "2023-09-13 19:16:56,867 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in /mnt/data/factcheck/wiki/pl/20230801/anserini/collection\n",
      "2023-09-13 19:16:56,868 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
      "2023-09-13 19:16:56,868 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-09-13 19:17:56,870 INFO  [main] index.IndexCollection (IndexCollection.java:534) - 460,000 documents indexed\n",
      "2023-09-13 19:18:56,871 INFO  [main] index.IndexCollection (IndexCollection.java:534) - 980,000 documents indexed\n",
      "2023-09-13 19:19:56,871 INFO  [main] index.IndexCollection (IndexCollection.java:534) - 1,540,000 documents indexed\n",
      "2023-09-13 19:20:56,872 INFO  [main] index.IndexCollection (IndexCollection.java:534) - 2,120,000 documents indexed\n",
      "2023-09-13 19:21:21,999 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection/docs.json: 2354793 docs added.\n",
      "2023-09-13 19:21:50,065 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 2,354,793 documents indexed\n",
      "2023-09-13 19:21:50,065 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-09-13 19:21:50,065 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:        2,354,793\n",
      "2023-09-13 19:21:50,065 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-09-13 19:21:50,065 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-09-13 19:21:50,065 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-09-13 19:21:50,065 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-09-13 19:21:50,068 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 2,354,793 documents indexed in 00:04:53\n"
     ]
    }
   ],
   "source": [
    "!python -m pyserini.index.lucene \\\n",
    "    -collection JsonCollection \\\n",
    "    -generator DefaultLuceneDocumentGenerator \\\n",
    "    -threads 4 \\\n",
    "    -input {ANSERINI_COLLECTION} \\\n",
    "    -language {ANSERINI_LANG} \\\n",
    "    -index {ANSERINI_INDEX} \\\n",
    "    -storePositions -storeDocvectors -storeRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/factcheck/wiki/pl/20230801/anserini/retrieved/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANSERINI_RETRIEVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28440/28440 [05:58<00:00, 79.38it/s]\n",
      "100%|██████████| 295209/295209 [1:01:02<00:00, 80.60it/s]\n"
     ]
    }
   ],
   "source": [
    "anserini_retrieve_claims(ANSERINI_INDEX, dev_data, 128)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, f\"dev_{APPROACH}.jsonl\"), dev_data, mkdir=True)\n",
    "\n",
    "anserini_retrieve_claims(ANSERINI_INDEX, tst_data, 128)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, f\"test_{APPROACH}.jsonl\"), tst_data, mkdir=True)\n",
    "\n",
    "anserini_retrieve_claims(ANSERINI_INDEX, trn_data, 128)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, f\"train_{APPROACH}.jsonl\"), trn_data, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = read_jsonl(Path(ANSERINI_RETRIEVED, f\"train_{APPROACH}.jsonl\"))\n",
    "dev_data = read_jsonl(Path(ANSERINI_RETRIEVED, f\"dev_{APPROACH}.jsonl\"))\n",
    "tst_data = read_jsonl(Path(ANSERINI_RETRIEVED, f\"test_{APPROACH}.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other jsonl claims: 30087 unique claims: 28476\n",
      "other jsonl claims: 28440 unique claims: 26991\n",
      "other jsonl claims: 295209 unique claims: 278746\n"
     ]
    }
   ],
   "source": [
    "def match_retrieval(data, other_jsonl):\n",
    "    odata = read_jsonl(other_jsonl)\n",
    "    claim2retrieved = {e[\"claim\"]: e[\"retrieved\"] for e in odata}\n",
    "    print(f\"other jsonl claims: {len(odata)} unique claims: {len(claim2retrieved)}\")\n",
    "    for e in data:\n",
    "        e[\"retrieved\"] = claim2retrieved[e[\"claim\"]]\n",
    "\n",
    "match_retrieval(dev_data, Path(ANSERINI_RETRIEVED, f\"dev_balanced.jsonl\"))\n",
    "match_retrieval(tst_data, Path(ANSERINI_RETRIEVED, f\"test_balanced.jsonl\"))\n",
    "match_retrieval(trn_data, Path(ANSERINI_RETRIEVED, f\"train_balanced.jsonl\"))\n",
    "\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, f\"dev_{APPROACH}.jsonl\"), dev_data, mkdir=True)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, f\"test_{APPROACH}.jsonl\"), tst_data, mkdir=True)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, f\"train_{APPROACH}.jsonl\"), trn_data, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/factcheck/wiki/pl/20230801/anserini/retrieved/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANSERINI_RETRIEVED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBERT Reranked Anserini\n",
    "\n",
    "Did not work well enough. Skip for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "231kit [00:09, 25.6kit/s] \n",
      "27.9kit [00:00, 36.8kit/s]\n"
     ]
    }
   ],
   "source": [
    "# import splits with hard negatives retrieved by Anserini\n",
    "trn_data_anserini = read_jsonl(Path(ANSERINI_RETRIEVED, \"train.jsonl\"), show_progress=True)\n",
    "dev_data_anserini = read_jsonl(Path(ANSERINI_RETRIEVED, \"dev.jsonl\"), show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27893/27893 [1:04:37<00:00,  7.19it/s]\n"
     ]
    }
   ],
   "source": [
    "sbert_CE_rerank(dev_data_anserini, corpus)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, \"dev_anserini+minilm.jsonl\"), dev_data_anserini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231296/231296 [9:09:20<00:00,  7.02it/s]  \n"
     ]
    }
   ],
   "source": [
    "sbert_CE_rerank(trn_data_anserini, corpus)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, \"train_anserini+minilm.jsonl\"), trn_data_anserini)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = read_jsonl(Path(ANSERINI_RETRIEVED, f\"train_{APPROACH}.jsonl\"))\n",
    "dev_data = read_jsonl(Path(ANSERINI_RETRIEVED, f\"dev_{APPROACH}.jsonl\"))\n",
    "tst_data = read_jsonl(Path(ANSERINI_RETRIEVED, f\"test_{APPROACH}.jsonl\"))\n",
    "# trn_data = read_jsonl(Path(ANSERINI_RETRIEVED, \"train_anserini+minilm.jsonl\"))\n",
    "# dev_data = read_jsonl(Path(ANSERINI_RETRIEVED, \"dev_anserini+minilm.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAIM: Leonardo Emilio Bonzi urodził się w Mediolanie.\n",
      "\n",
      "EVIDENCE (Leonardo_Bonzi_1) NOT FOUND 'Niles':\n",
      "Leonardo Bonzi  Leonardo Emilio Bonzi (ur. 22 grudnia 1902 w\n",
      "Mediolanie, zm. 30 grudnia 1977 w Ripalta Cremasca) – włoski\n",
      "bobsleista, olimpijczyk, a także reżyser i producent filmów\n",
      "dokumentalnych.  Był również tenisistą. W 1019 dotarł do 3. rundy gry\n",
      "pojedynczej French Championships i Wimbledonu.  Leonardo Bonzi\n",
      "\n",
      "RETRIEVED 1 (Leonardo_Bonzi_1) NOT FOUND 'Niles':\n",
      "Leonardo Bonzi  Leonardo Emilio Bonzi (ur. 22 grudnia 1902 w\n",
      "Mediolanie, zm. 30 grudnia 1977 w Ripalta Cremasca) – włoski\n",
      "bobsleista, olimpijczyk, a także reżyser i producent filmów\n",
      "dokumentalnych.  Był również tenisistą. W 1019 dotarł do 3. rundy gry\n",
      "pojedynczej French Championships i Wimbledonu.  Leonardo Bonzi\n",
      "\n",
      "RETRIEVED 2 (San_Leonardo_(stacja_metra)_1) NOT FOUND 'Niles':\n",
      "San Leonardo (stacja metra)  San Leonardo – stacja metra w Mediolanie,\n",
      "na linii M1. Znajduje się na via Fichera, w dzielnicy San Leonardo, w\n",
      "Mediolanie i zlokalizowana jest pomiędzy stacjami Bonola a Molino\n",
      "Dorino. Została otwarta w 1980.  San Leonardo (stacja metra)\n",
      "\n",
      "RETRIEVED 3 (Pierre_de_Bonzi_1) NOT FOUND 'Niles':\n",
      "Pierre de Bonzi  Pierre de Bonzi (ur. 15 kwietnia 1631 we Florencji,\n",
      "zm. 11 lipca 1703 w Montpellier) – francuski kardynał.  Życiorys.\n",
      "Urodził się 15 kwietnia 1631 roku we Florencji, jako syn Francesca\n",
      "Bonziego i Cristiny Riario. 7 czerwca 1660 roku został wybrany\n",
      "biskupem Béziers, a 12 grudnia przyjął sakrę. Pełnił funkcję\n",
      "ambasadora Francji w Toskanii (1661), Wenecji (1662–1668), Polsce\n",
      "(1669) i Hiszpanii (1670). W 1671 roku został arcybiskupem Tuluzy,\n",
      "otrzymując jednocześnie władzę nad Langwedocją, którą jednak utracił\n",
      "wskutek intryg wynikłych z jego relacji z panną de Gange. 22 lutego\n",
      "1672 roku został kreowany kardynałem prezbiterem i otrzymał kościół\n",
      "tytularny Sant’Onofrio. W 1674 roku został przeniesiony do\n",
      "archidiecezji Narbony. Zmarł 11 lipca 1703 roku w Montpellier.  Pierre\n",
      "de Bonzi\n"
     ]
    }
   ],
   "source": [
    "def show_retrieval(sample, corpus, search, k=3):\n",
    "    claim = sample[\"claim\"]\n",
    "    bid = sample[\"evidence\"][0]\n",
    "    rec = corpus[original_id2pid[bid]]\n",
    "    evidence = rec[\"text\"]\n",
    "    # retrieved = sample[\"retrieved\"]\n",
    "    print(f\"CLAIM: {claim}\")\n",
    "    print()\n",
    "\n",
    "    if search:\n",
    "        found = f\"FOUND '{search}'\" if search in evidence else f\"NOT FOUND '{search}'\"\n",
    "    else:\n",
    "        found = \"\"\n",
    "\n",
    "    print(f\"EVIDENCE ({bid}) {found}:\\n\" + textwrap.fill(evidence))\n",
    "\n",
    "    for i in range(k):\n",
    "        bid = sample[\"retrieved\"][i]\n",
    "        ret = corpus[original_id2pid[bid]][\"text\"]\n",
    "        if search:\n",
    "            found = f\"FOUND '{search}'\" if search in ret else f\"NOT FOUND '{search}'\"\n",
    "        else:\n",
    "            found = \"\"\n",
    "        print(f\"\\nRETRIEVED {i+1} ({bid}) {found}:\\n\" + textwrap.fill(ret))\n",
    "\n",
    "\n",
    "\n",
    "show_retrieval(tst_data[11], corpus, search=\"Niles\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triples by Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_preretrieve = 64\n",
    "offset = 0\n",
    "# trn_triples = generate_triples_by_retrieval(trn_data, corpus, original_id2pid, n_preretrieve, offset=offset)\n",
    "# dev_triples = generate_triples_by_retrieval(dev_data, corpus, original_id2pid, n_preretrieve, offset=offset)\n",
    "# write_jsonl(Path(COLBERT_ROOT, f\"train_triples{n_preretrieve}_o{offset}.jsonl\"), trn_triples)\n",
    "# write_jsonl(Path(COLBERT_ROOT, f\"dev_triples{n_preretrieve}_o{offset}.jsonl\"), dev_triples)\n",
    "# write_jsonl(Path(COLBERT_ROOT, f\"train_triples{n_preretrieve}_o{offset}_anserini+minilm.jsonl\"), trn_triples)\n",
    "# write_jsonl(Path(COLBERT_ROOT, f\"dev_triples{n_preretrieve}_o{offset}_anserini+minilm.jsonl\"), dev_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3784/183485 [00:00<00:20, 8949.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 103 fixing by random\n",
      "WARNING: not enough retrieved documents 128 => 116 fixing by random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 14785/183485 [00:01<00:15, 10581.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 108 fixing by random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 18948/183485 [00:01<00:15, 10584.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: more than 3 occurences of too few retrieved documents, fixing by random...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183485/183485 [00:21<00:00, 8683.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 183485 triples with 0 failures and 78 random fixes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 5006/18750 [00:00<00:01, 9232.34it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 45 fixing by random\n",
      "WARNING: not enough retrieved documents 128 => 35 fixing by random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 7406/18750 [00:00<00:01, 7735.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 54 fixing by random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 11191/18750 [00:01<00:00, 8530.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: more than 3 occurences of too few retrieved documents, fixing by random...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18750/18750 [00:02<00:00, 8336.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 18750 triples with 0 failures and 8 random fixes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1346/17727 [00:00<00:02, 6549.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 75 fixing by random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 8162/17727 [00:00<00:00, 10577.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 110 fixing by random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 12341/17727 [00:01<00:00, 9850.58it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 16 fixing by random\n",
      "WARNING: more than 3 occurences of too few retrieved documents, fixing by random...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17727/17727 [00:01<00:00, 9865.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 17727 triples with 0 failures and 5 random fixes\n"
     ]
    }
   ],
   "source": [
    "nway = 128\n",
    "\n",
    "trn_triples = generate_triples_by_retrieval_nway(trn_data, corpus, original_id2pid, nway=nway)\n",
    "write_jsonl(Path(TRIPLES_ROOT, f\"trn_triples_nway{nway}_anserini_{APPROACH}.jsonl\"), trn_triples, mkdir=True)\n",
    "\n",
    "dev_triples = generate_triples_by_retrieval_nway(dev_data, corpus, original_id2pid, nway=nway)\n",
    "write_jsonl(Path(TRIPLES_ROOT, f\"dev_triples_nway{nway}_anserini_{APPROACH}.jsonl\"), dev_triples, mkdir=True)\n",
    "\n",
    "tst_triples = generate_triples_by_retrieval_nway(tst_data, corpus, original_id2pid, nway=nway)\n",
    "write_jsonl(Path(TRIPLES_ROOT, f\"tst_triples_nway{nway}_anserini_{APPROACH}.jsonl\"), tst_triples, mkdir=True)\n",
    "\n",
    "# trn_triples = generate_triples_by_retrieval_nway(trn_data, corpus, original_id2pid, nway=nway)\n",
    "# write_jsonl(Path(COLBERT_ROOT, f\"trn_triples_nway{nway}_anserini+minilm.jsonl\"), trn_triples)\n",
    "\n",
    "# dev_triples = generate_triples_by_retrieval_nway(dev_data, corpus, original_id2pid, nway=nway)\n",
    "# write_jsonl(Path(COLBERT_ROOT, f\"dev_triples_nway{nway}_anserini+minilm.jsonl\"), dev_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2020/183204 [00:00<01:15, 2401.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 113 fixing by random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11114/183204 [00:02<01:02, 2764.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 31 fixing by random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 16090/183204 [00:03<00:52, 3189.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 125 fixing by random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 21206/183204 [00:04<00:22, 7099.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: more than 3 occurences of too few retrieved documents, fixing by random...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183204/183204 [00:38<00:00, 4805.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 183204 triples with 0 failures and 24 random fixes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 7744/18685 [00:01<00:03, 3206.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 57 fixing by random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 16047/18685 [00:03<00:01, 2296.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 99 fixing by random\n",
      "WARNING: not enough retrieved documents 128 => 1 fixing by random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18685/18685 [00:04<00:00, 4099.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 18685 triples with 0 failures and 3 random fixes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 641/17731 [00:00<00:16, 1043.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 115 fixing by random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17731/17731 [00:02<00:00, 6202.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 17731 triples with 0 failures and 1 random fixes\n"
     ]
    }
   ],
   "source": [
    "nway = 128\n",
    "\n",
    "trn_triples = generate_triples_by_retrieval_nway(trn_data, corpus, original_id2pid, nway=nway, use_evidence=True)\n",
    "write_jsonl(Path(TRIPLES_ROOT, f\"trn_triples_nway{nway}_evidence+anserini_{APPROACH}.jsonl\"), trn_triples, mkdir=True)\n",
    "\n",
    "dev_triples = generate_triples_by_retrieval_nway(dev_data, corpus, original_id2pid, nway=nway, use_evidence=True)\n",
    "write_jsonl(Path(TRIPLES_ROOT, f\"dev_triples_nway{nway}_evidence+anserini_{APPROACH}.jsonl\"), dev_triples, mkdir=True)\n",
    "\n",
    "tst_triples = generate_triples_by_retrieval_nway(tst_data, corpus, original_id2pid, nway=nway, use_evidence=True)\n",
    "write_jsonl(Path(TRIPLES_ROOT, f\"tst_triples_nway{nway}_evidence+anserini_{APPROACH}.jsonl\"), tst_triples, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/factcheck/wiki/pl/20230801/colbertv2/qacg/triples/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIPLES_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14084, 30087)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da = read_jsonl(\"/mnt/data/factcheck/wiki/sk/20230801/colbertv2/qacg/triples/crabz_slovakbert-ner/mt5-large_all-cp126k/mt5-large_all-cp156k/dev_triples_nway128_anserini_balanced.jsonl\")\n",
    "db = read_jsonl(\"/mnt/data/factcheck/wiki/sk/20230801/colbertv2/qacg/triples/crabz_slovakbert-ner/mt5-large_all-cp126k/mt5-large_all-cp156k/dev_triples_nway128_evidence+anserini_balanced.jsonl\")\n",
    "\n",
    "cnt = 0\n",
    "for a, b in zip(da, db):\n",
    "    a, b = np.array(a), np.array(b)\n",
    "    # print(len(a), len(b))\n",
    "    if not np.all(a == b) and len(a) == len(b):\n",
    "    # if len(a) != len(b):\n",
    "        # break\n",
    "        cnt += 1\n",
    "cnt, len(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30086, 152437, 155842, 155683, 155291, 188084, 293071, 164405,\n",
       "       177648, 194284, 156128, 153482, 155843, 177643, 151892, 159393,\n",
       "       293070, 152403, 162089, 160233, 177622, 157550, 152538, 194292,\n",
       "       157642, 155415, 154850, 154418, 194285, 155562, 153558, 194351,\n",
       "       193373, 194293, 155685, 155292, 156775, 151721, 177653, 193369,\n",
       "       156019, 293073, 193355, 194289, 193371, 194340, 256442, 194309,\n",
       "       156018, 155416, 167926, 155296, 201761, 194287, 193366, 151580,\n",
       "       155290, 193356, 194359, 264176, 194335, 250302, 194303, 194348,\n",
       "       194345, 157452, 156741, 160653, 177659, 151891, 253088, 194306,\n",
       "       155845, 194308, 253087,  98021, 194355, 154051, 281091, 156979,\n",
       "       202027, 201966, 293078, 291248, 177610, 154120, 201970, 161965,\n",
       "       250311, 152439, 193341, 160295, 154556, 160764, 201963, 152458,\n",
       "       159394, 154363, 155662, 201948, 201965, 155684, 201954, 293132,\n",
       "       201950, 154568, 194326, 152504, 167925, 344184, 202020,  98020,\n",
       "       293072, 157589, 157588,  97927, 201959, 156740,  97954, 201997,\n",
       "       151192, 177551, 180570, 151894, 157453, 200407, 155414, 293074,\n",
       "       278637])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30086, 155845, 152437, 155842, 155683, 155291, 188084, 293071,\n",
       "       164405, 177648, 194284, 156128, 153482, 155843, 177643, 151892,\n",
       "       159393, 293070, 152403, 162089, 160233, 177622, 157550, 152538,\n",
       "       194292, 157642, 155415, 154850, 154418, 194285, 155562, 153558,\n",
       "       194351, 193373, 194293, 155685, 155292, 156775, 151721, 177653,\n",
       "       193369, 156019, 293073, 193355, 194289, 193371, 194340, 256442,\n",
       "       194309, 156018, 155416, 167926, 155296, 201761, 194287, 193366,\n",
       "       151580, 155290, 193356, 194359, 264176, 194335, 250302, 194303,\n",
       "       194348, 194345, 157452, 156741, 160653, 177659, 151891, 253088,\n",
       "       194306, 194308, 253087,  98021, 194355, 154051, 281091, 156979,\n",
       "       202027, 201966, 293078, 291248, 177610, 154120, 201970, 161965,\n",
       "       250311, 152439, 193341, 160295, 154556, 160764, 201963, 152458,\n",
       "       159394, 154363, 155662, 201948, 201965, 155684, 201954, 293132,\n",
       "       201950, 154568, 194326, 152504, 167925, 344184, 202020,  98020,\n",
       "       293072, 157589, 157588,  97927, 201959, 156740,  97954, 201997,\n",
       "       151192, 177551, 180570, 151894, 157453, 200407, 155414, 293074,\n",
       "       278637])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'The_2nd_Law_4',\n",
       " 'did': 'The_2nd_Law',\n",
       " 'bid': 4,\n",
       " 'text': 'The 2nd Law\\n\\nAlbum vyšiel ako set súborov na stiahnutie, CD, bonusová edícia CD+DVD (so zábermi z výroby, \"The Making of The 2nd Law\", na DVD), a aj ako vinylová LP platňa. Edícia „Deluxe“ bola boxsetom \"The 2nd Law\" obsahujúcim CD, DVD vinylový dvojalbum a tri postery.\\n\\nThe 2nd Law\\n\\nPrezentácia albumu.\\n\\nDňa 16. júna 2012 kapela Muse vydala pre plánovaný album \"The 2nd Law\" trailer, ku ktorému bolo na ich oficiálnej webstránke zverejnené počítadlo, ktorého deň nula (t.j. plánované vydanie) pripadal na 17. september toho istého roku. Trailer obsahujúci aj dubstepové prvky sa stretol s rozpačitými reakciami skalných fanúšikov. Dňa 9. augusta dala kapela Muse pre fanúšikov, ktorí si objednali album k dispozícii nahrávku piesne „The 2nd Law: Unsustainable“. Dňa 10. augusta 2012, kapela zverejnila aj video k tejto piesni na svojej oficiálnej stránke na YouTube. Skupina zároveň urobil súťaž na produkciu hudobného video k nahrávke „Animals“. Víťazné video vyrobili Portugalčania, Inês Freitas a Miguel Mendes (Oneness Team). Víťazná nahrávka vyšla 20. marca 2013.',\n",
       " 'url': 'https://sk.wikipedia.org/wiki?curid=550351',\n",
       " 'revid': '107103'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[290620]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Muse_17',\n",
       " 'did': 'Muse',\n",
       " 'bid': 17,\n",
       " 'text': 'Muse\\n\\nDňa 6. júna 2012 skupina Muse vydala trailer nového albumu \"The 2nd Law\" s tým, že na svojom webe začali odpočítavať dni do 17. septembra, kedy by mali tento album vydať. Trailer, ktorý skutočne obsahoval aj dubstepové prvky bol od fanúšikov prijatý so zmiešanými reakciami.\\n\\nMuse\\n\\nNa 7. jún 2012 bol ohlásený prvý koncert The 2nd Law Tour. Turné sa začalo v Európe. Konalo sa vo Francúzsku, Španielsku, Spojenom kráľovstve a v ďalších krajinách. Prvý singel tohto albumu, „Survival“, bol oficiálnym songom Letných olympijských hier 2012, ktoré sa konali v Londýne. Singel mal spolu s prelúdiom premiéru v show, ktoré v BBC Radio 1 uvádzal DJ Zane Lowe. Taktiež s touto piesňou kapela Muse vystúpila počas záverečnej ceremónie týchto hier.\\n\\nMuse\\n\\nZoznam skladieb albumu \"The 2nd Law\" bol oficiálne zverejnený 13. júla 2012. Druhým jeho oficiálnym singlom bola nahrávka „Madness“. Vyšiel 20. augusta 2012 a hudobné video k tejto skladbe vyšlo 5. septembra. Prezentácia albumu bola dňa 30. septembra. Konala sa počas vystúpenia Muse na iTunes Festivale v londýnskej hale The Roundhouse. Samotný oficiálny svetový štart albumu \"The 2nd Law\" bol 1. október 2012, na druhý deň bol album daný na trhy v Spojených štátoch. Hneď po vydaní bol v rebríčku Spojeného kráľovstva, UK Albums Chart, prvý a v USA, v rebríčku \"Billboard\" 200, bol druhý. Song „Madness“ na 55.\\xa0ročníku ceny Grammy (2013) dostal nomináciu Best Rock Song a album bol nominovaný ako Best Rock Album. Dňa 20. februára 2013 pri príležitosti BRIT Awards, kapela Muse spolu s veľkým orchestrom zahrali úvodnú pieseň albumu: „Supremacy“. Inštrumentálne verzie dvoch piesní albumu: „The 2nd Law: Isolated System“ a „Follow Me“ boli niekoľkokrát použité ako scénická hudba k filmu \"Svetová vojna Z\". Herec v hlavnej postave a producent tohto apokalyptického filmu plného žijúcich mŕtvol, Brad Pitt, hovorí, že hľadal pieseň, ktorá by mohla byť prepojená s filmom podobne ako je ikonický projekt \"Tubular Bells\" od Mike Oldfielda spájaný s filmom \"Vyháňač diabla\" (\"The Exorcist\", 1973). A podľa Pitta boli tie dve skladby presne to, čo pre film hľadal.',\n",
       " 'url': 'https://sk.wikipedia.org/wiki?curid=151712',\n",
       " 'revid': '6357'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[104930]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Muse vydali album The 2nd Law.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = read_jsonl(\"/mnt/data/factcheck/wiki/sk/20230801/colbertv2/qacg/queries/crabz_slovakbert-ner/mt5-large_all-cp126k/mt5-large_all-cp156k/dev_qacg_queries_balanced.jsonl\")\n",
    "q[22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly sampled triples\n",
    "\n",
    "Not used..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145449/145449 [00:00<00:00, 543252.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found 0/263822 evidence documents, 0 claims had zero evidence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 779793.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found 0/14475 evidence documents, 0 claims had zero evidence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# replace with Anserini, random does not need \"retrieved\" field\n",
    "trn_data = import_split(Path(FEVER_ROOT, \"train.jsonl\"), original_id2pid, fixer=enfever_lrev_id_fixer)\n",
    "dev_data = import_split(Path(FEVER_ROOT, \"paper_dev.jsonl\"), original_id2pid, fixer=enfever_lrev_id_fixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_triples_random(data, corpus, original_id2pid, k, seed=1234):\n",
    "    # generate soft negatives by choosing random documents\n",
    "    idx2id = {i: doc[\"id\"] for i, doc in enumerate(corpus)}\n",
    "    id2idx = {doc[\"id\"]: i for i, doc in enumerate(corpus)}\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    def random_docs(posidx: int):\n",
    "        docs = set()\n",
    "        while len(docs) < k:\n",
    "            doc = rng.choice(len(id2idx))\n",
    "            if doc != pos and doc not in docs:\n",
    "                docs.add(doc)\n",
    "        return list(docs)\n",
    "\n",
    "    triples = []\n",
    "    for qid, r in enumerate(tqdm(data)):\n",
    "        for pos in r[\"evidence\"]:\n",
    "            posidx = id2idx[pos]\n",
    "            for neg in random_docs(posidx):\n",
    "                neg = idx2id[neg]\n",
    "                triples.append((qid, original_id2pid[pos], original_id2pid[neg]))\n",
    "    print(f\"generated {len(triples)} triples\")\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109810/109810 [00:41<00:00, 2625.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 4482720 triples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6666/6666 [00:02<00:00, 3034.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 258528 triples\n"
     ]
    }
   ],
   "source": [
    "k = 32\n",
    "trn_triples = generate_triples_random(trn_data, corpus, original_id2pid, k, seed=1234)\n",
    "dev_triples = generate_triples_random(dev_data, corpus, original_id2pid, k, seed=1235)\n",
    "tst_triples = generate_triples_random(tst_data, corpus, original_id2pid, k, seed=1236)\n",
    "write_jsonl(Path(COLBERT_ROOT, f\"train_triples{k}_random.jsonl\"), trn_triples)\n",
    "write_jsonl(Path(COLBERT_ROOT, f\"dev_triples{k}_random.jsonl\"), dev_triples)\n",
    "write_jsonl(Path(COLBERT_ROOT, f\"test_triples{k}_random.jsonl\"), tst_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement this, but for full Wikipedia corpora, EnFEVER has only single paragraph=document per page\n",
    "def generate_triples_by_page(data, corpus, original_id2pid, k):\n",
    "    # documents = paragraphs, I use page as term describing whole original text (e.g., Wikipedia page composed of documents=paragraphs)\n",
    "    # takes the positive document and adds k-1 negatives from the same page\n",
    "    id2txt = {doc[\"id\"]: doc[\"text\"] for doc in corpus}\n",
    "    failures = 0\n",
    "    triples = []\n",
    "    # for qid, r in enumerate(data):\n",
    "    #     # those retrieved but not in the annotated evidence will become hard negatives \n",
    "    #     retrieved = set(r[\"retrieved\"][offset:]).difference(r[\"evidence\"])\n",
    "    #     for pos in r[\"evidence\"]:\n",
    "    #         if pos not in id2txt:\n",
    "    #             # may happen for EnFEVER when the snapshot does not exactly match \n",
    "    #             failures += 1\n",
    "    #             continue\n",
    "    #         for neg in list(retrieved)[:k]:\n",
    "    #             triples.append((qid, original_id2pid[pos], original_id2pid[neg]))\n",
    "    # print(f\"generated {len(triples)} triples with {failures} failures\")\n",
    "    return triples\n",
    "\n",
    "# k = 8\n",
    "# dev_triples = generate_triples_by_page(dev_data, corpus, original_id2pid, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2018–19_Dhaka_Premier_Division_Twenty20_Cricket_League',\n",
       " 'text': 'The 2018–19 Dhaka Premier Division Twenty20 Cricket League was the first edition of the Dhaka Premier Division Twenty20 Cricket League, a Twenty20 cricket competition that was held in Bangladesh. It started on 25 February 2019 and concluded on 4 March 2019. The tournament took place directly before the 2018–19 Dhaka Premier Division Cricket League, and features the same twelve teams. The final of the competition was played as a night game at the Sher-e-Bangla National Stadium in Mirpur. The Bangladesh Cricket Board (BCB) instigated the tournament in order to give Bangladeshi players more experience in the 20-over format, in the hope that local players will become more prominent in the Bangladesh Premier League. For this reason the tournament featured local cricketers exclusively, unlike the Dhaka Premier Division Cricket League, in which foreign players take part. Shinepukur Cricket Club were the first team to qualify for the semi-finals of the tournament, after they won their two matches in Group C. On the final day of the group stage, Prime Bank Cricket Club from Group A and Sheikh Jamal Dhanmondi Club from Group B also progressed to the semi-finals. The final match in Group D, between Gazi Group Cricketers and Prime Doleshwar Sporting Club was washed out, so the fixture was rescheduled for the reserve day to determine which team progresses. Prime Doleshwar Sporting Club won the rescheduled match by three wickets, winning Group D and becoming the fourth team to advance to the semi-finals. In the first semi-final, Sheikh Jamal Dhanmondi Club beat Shinepukur Cricket Club by five wickets, with man of the match Ziaur Rahman finishing unbeaten on 72 runs from just 29 balls. The second semi-final saw Prime Doleshwar Sporting Club beat Prime Bank Cricket Club by six wickets, with their captain, Farhad Reza, taking five wickets for 32 runs. Sheikh Jamal Dhanmondi Club won the tournament, after they beat Prime Doleshwar Sporting Club by 24 runs in the final. Imtiaz Hossain was named as the man of the match, after scoring 56 runs, and Farhad Reza was named as the player of the tournament, following his all-round performance. Following the conclusion of the competition, the BCB confirmed their intentions to have the tournament as a permanent fixture in their domestic calendar.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Snoop_Dogg', 6912),\n",
       " ('Marlon_Brando', 6880),\n",
       " ('United_States', 6112),\n",
       " ('Wyatt_Earp', 5888),\n",
       " ('Michael_Jackson', 5856),\n",
       " ('United_Kingdom', 5760),\n",
       " ('Adele', 5152),\n",
       " ('Miley_Cyrus', 5024),\n",
       " ('Tim_Rice', 4832),\n",
       " ('The_Beatles', 4544),\n",
       " ('International_relations', 4480),\n",
       " ('A_Song_of_Ice_and_Fire', 4480),\n",
       " ('Abraham_Lincoln', 4448),\n",
       " ('David_Beckham', 4320),\n",
       " ('Anne_Hathaway', 4192),\n",
       " ('One_Direction', 4192),\n",
       " ('Frank_Sinatra', 4160),\n",
       " ('Oliver_Reed', 4096),\n",
       " ('Deadpool_-LRB-_film_-RRB-', 4064),\n",
       " ('Bradley_Cooper', 4064)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter(pid2original_id[t[1]] for t in trn_triples)\n",
    "c.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(Path(COLBERT_ROOT, f\"train_triples{n_preretrieve}_o{offset}.jsonl\"), trn_triples)\n",
    "write_jsonl(Path(COLBERT_ROOT, f\"dev_triples{n_preretrieve}_o{offset}.jsonl\"), dev_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_triples = read_jsonl(Path(COLBERT_ROOT, f\"dev_triples1_random.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telemundo is a English-language television network.\n",
      "Hispanic_and_Latino_Americans\n",
      "\n",
      "Cnaphalocrocis_poeyalis\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "for qid, pos, neg in dev_triples[1:]:\n",
    "    claim = dev_data[qid]['claim']\n",
    "    # pos = textwrap.fill(corpus[pos]['text'])\n",
    "    # neg = textwrap.fill(corpus[neg]['text'])\n",
    "    pos = corpus[pos][\"id\"]\n",
    "    neg = corpus[neg][\"id\"]\n",
    "\n",
    "    print(f\"{claim}\")\n",
    "    print(f\"{pos}\")\n",
    "    print()\n",
    "    print(f\"{neg}\")\n",
    "    print(\"-----------\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_queries_triples_collection(\n",
    "        split_files, # already created combined split\n",
    "        triple_files, # target files for triples (keep same order of splits!)\n",
    "        query_files, # target files for queries (keep same order of splits!)\n",
    "        collection_dir, # combined collection dir\n",
    "        lang2triple_files, # triple files for each particular source language\n",
    "        lang2collection_files\n",
    "        ):\n",
    "    \n",
    "    #import splits and generate queries (just claims) \n",
    "    splits = []\n",
    "    for split_file, query_file in zip(split_files, query_files):\n",
    "        data = read_jsonl(split_file)\n",
    "        splits.append(data)\n",
    "        recs = [{\"query\": r[\"claim\"]} for r in data]\n",
    "        print(f'writing query file: \"{query_file}\"')\n",
    "        write_jsonl(query_file, recs, mkdir=True)\n",
    "\n",
    "    print(\"loading collections\")\n",
    "    lang2collection = {lang: read_jsonl(collection_file, show_progress=True) for lang, collection_file in lang2collection_files.items()}\n",
    "\n",
    "    print(\"loading triples\")\n",
    "    lang2triples = {lang: [read_jsonl(Path(triple_file_lst[0], triple_file_name)) for triple_file_name in triple_file_lst[1]] for lang, triple_file_lst in lang2triple_files.items()}\n",
    "\n",
    "    new_collection = []\n",
    "    used_collection_pids = defaultdict(dict) # language to original pid to new pid \n",
    "    new_triples = [[] for _ in range(len(splits))] # dict of list (splits) of lists (triples)\n",
    "\n",
    "    for split_idx, split in enumerate(splits):\n",
    "        # run through the matching split and select all samples with matching `lang`\n",
    "        for s in tqdm(split, desc=f\"split idx: {split_idx}\"):\n",
    "            # print(f\"DEBUG: sample={s}\")\n",
    "            lang = s[\"lang\"]\n",
    "            # index from the original language split\n",
    "            orig_idx = int(s[\"orig_idx\"])\n",
    "            # print(f\"DEBUG: orig_idx={orig_idx}\")\n",
    "            trp = lang2triples[lang][split_idx][orig_idx]\n",
    "            # print(f\"DEBUG: trp={trp}\")\n",
    "            new_trp_id = len(new_triples[split_idx]) # generate ever increasing triple ids\n",
    "            new_trp = [new_trp_id]\n",
    "            # now translate original pids to new pids\n",
    "            for orig_pid in trp[1:]:\n",
    "                if orig_pid not in used_collection_pids[lang]:\n",
    "                    new_pid = len(new_collection) # ever increasing pid given by position in `the new_collection`\n",
    "                    used_collection_pids[lang][orig_pid] = new_pid\n",
    "                    col_item = lang2collection[lang][orig_pid]\n",
    "                    new_collection.append(col_item)\n",
    "                else:\n",
    "                    new_pid = used_collection_pids[lang][orig_pid]\n",
    "                new_trp.append(new_pid)\n",
    "            # print(f\"DEBUG: new_trp={new_trp}\")\n",
    "                    \n",
    "            new_triples[split_idx].append(new_trp)\n",
    "    \n",
    "    assert len(new_triples) == len(triple_files)\n",
    "    for nt, triple_file in zip(new_triples, triple_files):\n",
    "        print(f'writing triple file: \"{triple_file}\"')\n",
    "        write_jsonl(triple_file, nt, mkdir=True)\n",
    "\n",
    "    collection_file = Path(collection_dir, \"collection.jsonl\")\n",
    "    print(f'writing collection file: \"{collection_file}\"')\n",
    "    write_jsonl(collection_file, new_collection, mkdir=True, show_progress=True)\n",
    "\n",
    "    # Most likely makes no sense as the bids from different Wikipedia may overlap \n",
    "    # original_id2pid = {r[\"id\"]: i for i, r in enumerate(new_collection)}\n",
    "    # original_id2pid_file = Path(collection_dir, \"original_id2pid.json\")\n",
    "    # print(f'writing original_id2pid file: \"{original_id2pid_file}\"')\n",
    "    # write_json(original_id2pid_file, original_id2pid, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing query file: \"/mnt/data/factcheck/wiki/cs_en_pl_sk/20230801/colbertv2/qacg/queries/dev_qacg_queries_balanced_shuf.jsonl\"\n",
      "writing query file: \"/mnt/data/factcheck/wiki/cs_en_pl_sk/20230801/colbertv2/qacg/queries/test_qacg_queries_balanced_shuf.jsonl\"\n",
      "writing query file: \"/mnt/data/factcheck/wiki/cs_en_pl_sk/20230801/colbertv2/qacg/queries/train_qacg_queries_balanced_shuf.jsonl\"\n",
      "loading collections\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009025096893310547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13a20a5c8c447fba415eb252cca2d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008198022842407227,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d1c354bb9744658b1827083a956767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008955717086791992,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9f207414ec4084a8f836affcf57e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009008169174194336,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06896f92b198445aaeb7ad00e2967ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading triples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "split idx: 0: 100%|██████████| 18750/18750 [00:01<00:00, 12830.77it/s]\n",
      "split idx: 1: 100%|██████████| 18146/18146 [00:01<00:00, 12137.26it/s]\n",
      "split idx: 2: 100%|██████████| 186489/186489 [00:14<00:00, 12693.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing triple file: \"/mnt/data/factcheck/wiki/cs_en_pl_sk/20230801/colbertv2/qacg/triples/dev_triples_nway128_evidence+anserini_balanced_shuf.jsonl\"\n",
      "writing triple file: \"/mnt/data/factcheck/wiki/cs_en_pl_sk/20230801/colbertv2/qacg/triples/tst_triples_nway128_evidence+anserini_balanced_shuf.jsonl\"\n",
      "writing triple file: \"/mnt/data/factcheck/wiki/cs_en_pl_sk/20230801/colbertv2/qacg/triples/trn_triples_nway128_evidence+anserini_balanced_shuf.jsonl\"\n",
      "writing collection file: \"/mnt/data/factcheck/wiki/cs_en_pl_sk/20230801/colbertv2/qacg/collection.jsonl\"\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008975505828857422,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 7450195,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d3c7ec781940079223dd8307c0dbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7450195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "APPROACH=\"balanced_shuf\"\n",
    "DATE = \"20230801\"\n",
    "QACG_ROOT=f\"/mnt/data/factcheck/wiki/cs_en_pl_sk/{DATE}/qacg\"\n",
    "COLBERT_ROOT=f\"/mnt/data/factcheck/wiki/cs_en_pl_sk/{DATE}/colbertv2/qacg\"\n",
    "COLBERT_ROOT_CS=f\"/mnt/data/factcheck/wiki/cs/{DATE}/colbertv2/qacg\"\n",
    "COLBERT_ROOT_EN=f\"/mnt/data/factcheck/wiki/en/{DATE}/colbertv2/qacg\"\n",
    "COLBERT_ROOT_PL=f\"/mnt/data/factcheck/wiki/pl/{DATE}/colbertv2/qacg\"\n",
    "COLBERT_ROOT_SK=f\"/mnt/data/factcheck/wiki/sk/{DATE}/colbertv2/qacg\"\n",
    "MODELS_CS=f\"PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k\"\n",
    "MODELS_EN=f\"stanza/mt5-large_all-cp126k/mt5-large_all-cp156k\"\n",
    "MODELS_PL=f\"stanza/mt5-large_all-cp126k/mt5-large_all-cp156k\"\n",
    "MODELS_SK=f\"crabz_slovakbert-ner/mt5-large_all-cp126k/mt5-large_all-cp156k\"\n",
    "\n",
    "# TRIPLE_SPLITS = [\n",
    "#     f\"dev_triples_nway128_anserini_{APPROACH}.jsonl\",\n",
    "#     f\"tst_triples_nway128_anserini_{APPROACH}.jsonl\",\n",
    "#     f\"trn_triples_nway128_anserini_{APPROACH}.jsonl\",\n",
    "#     ]\n",
    "\n",
    "TRIPLE_SPLITS = [\n",
    "    f\"dev_triples_nway128_evidence+anserini_{APPROACH}.jsonl\",\n",
    "    f\"tst_triples_nway128_evidence+anserini_{APPROACH}.jsonl\",\n",
    "    f\"trn_triples_nway128_evidence+anserini_{APPROACH}.jsonl\",\n",
    "    ]\n",
    "\n",
    "combine_queries_triples_collection(\n",
    "    split_files=[\n",
    "        f\"{QACG_ROOT}/splits/dev_{APPROACH}.jsonl\",\n",
    "        f\"{QACG_ROOT}/splits/test_{APPROACH}.jsonl\",\n",
    "        f\"{QACG_ROOT}/splits/train_{APPROACH}.jsonl\",\n",
    "    ],\n",
    "    query_files=[\n",
    "        f\"{COLBERT_ROOT}/queries/dev_qacg_queries_{APPROACH}.jsonl\",\n",
    "        f\"{COLBERT_ROOT}/queries/test_qacg_queries_{APPROACH}.jsonl\",\n",
    "        f\"{COLBERT_ROOT}/queries/train_qacg_queries_{APPROACH}.jsonl\",\n",
    "    ],\n",
    "    triple_files=[f\"{COLBERT_ROOT}/triples/{ts}\" for ts in TRIPLE_SPLITS],\n",
    "    collection_dir=COLBERT_ROOT,\n",
    "    lang2triple_files = {\n",
    "        \"cs\": (f\"{COLBERT_ROOT_CS}/triples/{MODELS_CS}\", TRIPLE_SPLITS),\n",
    "        \"en\": (f\"{COLBERT_ROOT_EN}/triples/{MODELS_EN}\", TRIPLE_SPLITS),\n",
    "        \"pl\": (f\"{COLBERT_ROOT_PL}/triples/{MODELS_PL}\", TRIPLE_SPLITS),\n",
    "        \"sk\": (f\"{COLBERT_ROOT_SK}/triples/{MODELS_SK}\", TRIPLE_SPLITS),\n",
    "    },\n",
    "    lang2collection_files = {\n",
    "        \"cs\": f\"{COLBERT_ROOT_CS}/collection.jsonl\",\n",
    "        \"en\": f\"{COLBERT_ROOT_EN}/collection.jsonl\",\n",
    "        \"pl\": f\"{COLBERT_ROOT_PL}/collection.jsonl\",\n",
    "        \"sk\": f\"{COLBERT_ROOT_SK}/collection.jsonl\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/data/factcheck/wiki/cs_en_pl_sk/20230801/qacg'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QACG_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing query file: \"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/20230801/colbertv2/qacg/queries/dev_qacg_queries_balanced_shuf.jsonl\"\n",
      "writing query file: \"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/20230801/colbertv2/qacg/queries/test_qacg_queries_balanced_shuf.jsonl\"\n",
      "writing query file: \"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/20230801/colbertv2/qacg/queries/train_qacg_queries_balanced_shuf.jsonl\"\n",
      "loading collections\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008528709411621094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d91ab1b68345cba21463910b0b9f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010975360870361328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c7a46deb5341d3aaa9a3a98b9a1e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00929880142211914,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8dd01787d647b292f283c63a186e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009390115737915039,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5589e131ef6a4b49bd4115fe75329119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading triples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "split idx: 0: 100%|██████████| 75573/75573 [00:05<00:00, 13991.37it/s]\n",
      "split idx: 1: 100%|██████████| 71607/71607 [00:05<00:00, 13119.88it/s]\n",
      "split idx: 2: 100%|██████████| 741542/741542 [00:56<00:00, 13156.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing triple file: \"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/20230801/colbertv2/qacg/triples/dev_triples_nway128_evidence+anserini_balanced_shuf.jsonl\"\n",
      "writing triple file: \"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/20230801/colbertv2/qacg/triples/tst_triples_nway128_evidence+anserini_balanced_shuf.jsonl\"\n",
      "writing triple file: \"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/20230801/colbertv2/qacg/triples/trn_triples_nway128_evidence+anserini_balanced_shuf.jsonl\"\n",
      "writing collection file: \"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/20230801/colbertv2/qacg/collection.jsonl\"\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009328126907348633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 12243036,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4541396dbb0a4277b6b3992ce8efc3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12243036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the same for the language SUM dataset\n",
    "APPROACH=\"balanced_shuf\"\n",
    "DATE = \"20230801\"\n",
    "QACG_ROOT=f\"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/{DATE}/qacg\"\n",
    "COLBERT_ROOT=f\"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/{DATE}/colbertv2/qacg\"\n",
    "COLBERT_ROOT_CS=f\"/mnt/data/factcheck/wiki/cs/{DATE}/colbertv2/qacg\"\n",
    "COLBERT_ROOT_EN=f\"/mnt/data/factcheck/wiki/en/{DATE}/colbertv2/qacg\"\n",
    "COLBERT_ROOT_PL=f\"/mnt/data/factcheck/wiki/pl/{DATE}/colbertv2/qacg\"\n",
    "COLBERT_ROOT_SK=f\"/mnt/data/factcheck/wiki/sk/{DATE}/colbertv2/qacg\"\n",
    "MODELS_CS=f\"PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k\"\n",
    "MODELS_EN=f\"stanza/mt5-large_all-cp126k/mt5-large_all-cp156k\"\n",
    "MODELS_PL=f\"stanza/mt5-large_all-cp126k/mt5-large_all-cp156k\"\n",
    "MODELS_SK=f\"crabz_slovakbert-ner/mt5-large_all-cp126k/mt5-large_all-cp156k\"\n",
    "# TRIPLE_SPLITS = [\n",
    "#     f\"dev_triples_nway128_anserini_{APPROACH}.jsonl\",\n",
    "#     f\"tst_triples_nway128_anserini_{APPROACH}.jsonl\",\n",
    "#     f\"trn_triples_nway128_anserini_{APPROACH}.jsonl\",\n",
    "#     ]\n",
    "\n",
    "TRIPLE_SPLITS = [\n",
    "    f\"dev_triples_nway128_evidence+anserini_{APPROACH}.jsonl\",\n",
    "    f\"tst_triples_nway128_evidence+anserini_{APPROACH}.jsonl\",\n",
    "    f\"trn_triples_nway128_evidence+anserini_{APPROACH}.jsonl\",\n",
    "    ]\n",
    "\n",
    "combine_queries_triples_collection(\n",
    "    split_files=[\n",
    "        f\"{QACG_ROOT}/splits/dev_{APPROACH}.jsonl\",\n",
    "        f\"{QACG_ROOT}/splits/test_{APPROACH}.jsonl\",\n",
    "        f\"{QACG_ROOT}/splits/train_{APPROACH}.jsonl\",\n",
    "    ],\n",
    "    query_files=[\n",
    "        f\"{COLBERT_ROOT}/queries/dev_qacg_queries_{APPROACH}.jsonl\",\n",
    "        f\"{COLBERT_ROOT}/queries/test_qacg_queries_{APPROACH}.jsonl\",\n",
    "        f\"{COLBERT_ROOT}/queries/train_qacg_queries_{APPROACH}.jsonl\",\n",
    "    ],\n",
    "    triple_files=[f\"{COLBERT_ROOT}/triples/{ts}\" for ts in TRIPLE_SPLITS],\n",
    "    collection_dir=COLBERT_ROOT,\n",
    "    lang2triple_files = {\n",
    "        \"cs\": (f\"{COLBERT_ROOT_CS}/triples/{MODELS_CS}\", TRIPLE_SPLITS),\n",
    "        \"en\": (f\"{COLBERT_ROOT_EN}/triples/{MODELS_EN}\", TRIPLE_SPLITS),\n",
    "        \"pl\": (f\"{COLBERT_ROOT_PL}/triples/{MODELS_PL}\", TRIPLE_SPLITS),\n",
    "        \"sk\": (f\"{COLBERT_ROOT_SK}/triples/{MODELS_SK}\", TRIPLE_SPLITS),\n",
    "    },\n",
    "    lang2collection_files = {\n",
    "        \"cs\": f\"{COLBERT_ROOT_CS}/collection.jsonl\",\n",
    "        \"en\": f\"{COLBERT_ROOT_EN}/collection.jsonl\",\n",
    "        \"pl\": f\"{COLBERT_ROOT_PL}/collection.jsonl\",\n",
    "        \"sk\": f\"{COLBERT_ROOT_SK}/collection.jsonl\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/20230801/colbertv2/qacg'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLBERT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.023613929748535156,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835cb3cf05a040169c90fc2a163443a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = import_corpus(\"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/20230801/colbertv2/qacg/collection.jsonl\")\n",
    "triples = read_jsonl(\"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/20230801/colbertv2/qacg/triples/dev_triples_nway128_anserini_balanced.jsonl\")\n",
    "queries = read_jsonl(\"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/20230801/colbertv2/qacg/queries/dev_qacg_queries_balanced.jsonl\")\n",
    "\n",
    "# corpus = import_corpus(\"/mnt/data/factcheck/wiki/sk/20230801/colbertv2/qacg/collection.jsonl\")\n",
    "# triples = read_jsonl(\"/mnt/data/factcheck/wiki/sk/20230801/colbertv2/qacg/triples/crabz_slovakbert-ner/mt5-large_all-cp126k/mt5-large_all-cp156k/dev_triples_nway128_anserini_balanced.jsonl\")\n",
    "# queries = read_jsonl(\"/mnt/data/factcheck/wiki/sk/20230801/colbertv2/qacg/queries/crabz_slovakbert-ner/mt5-large_all-cp126k/mt5-large_all-cp156k/dev_qacg_queries_balanced.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383]\n",
      "{'query': 'Najbližšia železničná stanica v Žerčice je Dobrovice.'}\n",
      "{'id': 'Žerčice_2', 'did': 'Žerčice', 'bid': 2, 'text': 'Žerčice\\n\\nvýroba cementového tovaru, holič, 2 hostince, kolár, 2 kováči, mlyn, obchod s obuvou Baťa, 3 obuvníci, pekár, 2 mäsiari, 3 obchody so zmiešaným tovarom, sporiteľní a záložní spolok pre Žerčice, trafika, 3 stolári, veľkostatok\\n\\nDo obce vedú cesty III. triedy\\n\\nŽerčice\\n\\nŽelezničná trať ani stanica na území obce v súčasnosti niesu.\\n\\nNajbližšie obci je železničná stanica Dobrovice vo vzdialenosti 6\\xa0km ležiaca na trati 071 z Nymburka do Mladej Boleslavi.\\n\\nV minulosti Žerčicami viedla železničná trať Rokytňany - Dobrovice město.\\n\\nZrušená železničná trať bola jednokoľajová miestna súkromná trať, majiteľom bol gróf Thurn-Taxis. Nákladná doprava bola zahájená roku 1883, osobná doprava roku 1902. Trať bola zoštátnená roku 1908. Prepravné zaťaženie trate vlakmi pre cestujúcich bolo minimálne, išlo o 2 páry osobných vlakov denne. Osobná doprava bola zastavená roku 1970, trať zrušená roku 1974.\\n\\nŽerčice\\n\\nV obci mali zastávku v júni 2011 autobusové linky idúce do týchto cieľov: Dobrovice, Jičín, Mladá Boleslav, Nová Paka, Pecka, Praha.', 'url': 'https://sk.wikipedia.org/wiki?curid=170959', 'revid': '78308'}\n",
      "{'id': 'Standford_1', 'did': 'Standford', 'bid': 1, 'text': 'Standford\\n\\nStandford je obec v\\xa0okrsku Headley v\\xa0okrese East Hampshire v\\xa0grófstve Hampshire v\\xa0Anglicku v\\xa0Spojenom kráľovstve.\\n\\nLeží na ceste B3004, približne 2 km východne od mesta Bordon. V\\xa0dedine sa nachádza autobusová zastávka a\\xa0najbližšia železničná stanica je v dedine Liphook (pozri Železničná stanica Liphook), približne 4,5 km juhovýchodne.\\n\\nStandford\\n\\nZdroj.\\n\\n\"Tento článok je čiastočný alebo úplný preklad článku [ Standford] na anglickej Wikipédii.\"', 'url': 'https://sk.wikipedia.org/wiki?curid=600967', 'revid': '2483'}\n",
      "{'id': 'Železničná_stanica_Bohumín_1', 'did': 'Železničná_stanica_Bohumín', 'bid': 1, 'text': 'Železničná stanica Bohumín\\n\\nŽelezničná stanica Bohumín je železničná stanica v rovnomennom meste na adrese Adama Mickiewicze 67, ktoré sa nachádza v Moravskoslezskom kraji, asi 10 km severne od Ostravy, v blízkosti hraníc Poľska a Česka.\\n\\nDejiny.\\n\\nPrvý vlak do Bohumína prišiel už 1. mája 1847, kedy bola otvorená Severná železnica cisára Ferdinanda (KFNB) z Viedne, čím sa pre krajinu sprístupnili sliezske bane. Krátkym spojovacím úsekom do pruského Annabergu (dnes Chałupki) bola 1. septembra 1848 KFNB napojená na pruskú trať, čím vzniklo železničné spojenie Viedne s Berlínom. Spojenie do Krakova bolo dosiahnuté použitím Krakovsko-hornosliezskej železnice (vedúcej pruským územím) a KFNB tak stačilo postaviť len trať z Bohumína do Osvienčimu, kde bola najbližšia zastávka už existujúcej trate. Celá trať z Viedne do Krakova tak bola sprejazdnená 1. marca 1856.\\n\\nŽelezničná stanica Bohumín\\n\\nUž tak dôležitá stanica sa medzi významné dopravné uzly zaradila v roku 1869, kedy bol otvorený prvý úsek Košicko-bohumínskej železnice po Třinec. Po dobudovaní trate do Košíc v roku 1872 sa tak Bohumín stal cieľovou stanicou vlakov zo Slovenska a prestupnou stanicou cestujúcich v smere na Viedeň, Krakov i Berlín.', 'url': 'https://sk.wikipedia.org/wiki?curid=525613', 'revid': '178265'}\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "print(triples[idx])\n",
    "print(queries[idx])\n",
    "print(corpus[triples[idx][1]])\n",
    "print(corpus[triples[idx][2]])\n",
    "print(corpus[triples[idx][3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lang'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/drchajan/devel/python/FC/ColBERTv2/notebooks/prepare_data_wiki.ipynb Cell 47\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Brci/home/drchajan/devel/python/FC/ColBERTv2/notebooks/prepare_data_wiki.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m Counter([r[\u001b[39m\"\u001b[39m\u001b[39mlang\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m corpus])\n",
      "\u001b[1;32m/home/drchajan/devel/python/FC/ColBERTv2/notebooks/prepare_data_wiki.ipynb Cell 47\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Brci/home/drchajan/devel/python/FC/ColBERTv2/notebooks/prepare_data_wiki.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m Counter([r[\u001b[39m\"\u001b[39;49m\u001b[39mlang\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m corpus])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lang'"
     ]
    }
   ],
   "source": [
    "Counter([r[\"lang\"] for r in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Medzinárodné_letisko_Vancouver_4',\n",
       " 'did': 'Medzinárodné_letisko_Vancouver',\n",
       " 'bid': 4,\n",
       " 'text': 'Medzinárodné letisko Vancouver\\n\\nLetisko získalo v roku 2001 ocenenie \"Airport Management Award\" od B.C. Aviation Council.\\n\\nMedzinárodné letisko Vancouver\\n\\nZdroje.\\n\\n\"Tento článok je čiastočný alebo úplný preklad článku [ Vancouver International Airport] na anglickej Wikipédii.\"',\n",
       " 'url': 'https://sk.wikipedia.org/wiki?curid=222084',\n",
       " 'revid': '6909'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sk[146171]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 03, 08:19:36] #> Loading collection from JSONL...\n",
      "0M 1M 2M 3M 4M 5M \n",
      "ColBERT: self.colbert_config=ColBERTConfig(query_token_id='[unused0]', doc_token_id='[unused1]', query_token='[Q]', doc_token='[D]', ncells=None, centroid_score_threshold=None, ndocs=None, index_path=None, nbits=2, kmeans_niters=4, resume=False, similarity='cosine', bsize=64, accumsteps=8, lr=3e-06, maxsteps=500000, save_every=None, warmup=None, warmup_bert=None, relu=False, nway=32, use_ib_negatives=False, reranker=False, distillation_alpha=1.0, ignore_scores=False, model_name='deepset/xlm-roberta-large-squad2', query_maxlen=32, attend_to_mask_tokens=False, interaction='colbert', dim=128, doc_maxlen=300, mask_punctuation=True, checkpoint='/mnt/data/factcheck/fever/data-en-lrev/colbertv2/checkpoints/xlm-roberta-large-squad2/enfever_lrev/train_nway32_anserini+minilm/colbert', triples='/mnt/data/factcheck/fever/data-en-lrev/colbertv2/trn_triples_nway128_anserini+minilm.jsonl', collection=<colbert.data.collection.Collection object at 0x7f34182d0730>, queries='/mnt/data/factcheck/fever/data-en-lrev/colbertv2/train_queries.jsonl', index_name='/mnt/data/factcheck/fever/data-en-lrev/colbertv2/indices/xlm-roberta-large-squad2/enfever_lrev/nway32_anserini+minilm.2bits', overwrite=False, root='/home/drchajan/devel/python/FC/ColBERTv2/experiments', experiment='FEVER predictions', index_root=None, name='2023-04/03/08.19.09', rank=0, nranks=1, amp=True, gpus=1)\n",
      "[Apr 03, 08:20:16] #> Loading codec...\n",
      "[Apr 03, 08:20:16] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Apr 03, 08:20:17] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Apr 03, 08:20:18] #> Loading IVF...\n",
      "[Apr 03, 08:20:18] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:00<00:00, 784.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 03, 08:20:19] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:09<00:00, 23.28it/s]\n"
     ]
    }
   ],
   "source": [
    "class ColBERTv2Retriever:\n",
    "    def __init__(self, index_name, original_id2pid_file):\n",
    "            with Run().context(RunConfig(experiment='FEVER predictions')):\n",
    "                self.searcher = Searcher(index=index_name)\n",
    "            self.original_id2pid = read_json(original_id2pid_file)\n",
    "            self.pid2original_id = {v: k for k, v in self.original_id2pid.items()}\n",
    "\n",
    "    def retrieve(self, query: str, k: int):\n",
    "        results = self.searcher.search(query, k=k)\n",
    "        pids, ranks, scores = results\n",
    "        ids = [self.pid2original_id[pid] for pid in pids]\n",
    "        return ids, scores\n",
    "    \n",
    "# IDX_NAME = \"/mnt/data/factcheck/fever/data-en-lrev/colbertv2-sqlite/indices/bert-base-uncased/msmarco/msmarco.2bits\"\n",
    "# IDX_NAME = \"/mnt/data/factcheck/fever/data-en-lrev/colbertv2/indices/bert-base-multilingual-cased/enfever_lrev/triples32_random_cp20k.2bits\"\n",
    "# IDX_NAME = \"/mnt/data/factcheck/fever/data-en-lrev/colbertv2/indices/bert-base-multilingual-cased/enfever_lrev/triples64_o0_anserini+minilm_cp10k.2bits\"\n",
    "# IDX_NAME = \"/mnt/data/factcheck/fever/data-en-lrev/colbertv2/indices/bert-base-multilingual-cased/enfever_lrev/triples64_o0_anserini+minilm_cp30k.2bits\"\n",
    "IDX_NAME = \"/mnt/data/factcheck/fever/data-en-lrev/colbertv2/indices/xlm-roberta-large-squad2/enfever_lrev/nway32_anserini+minilm.2bits\"\n",
    "# IDX_NAME = \"/mnt/data/factcheck/fever/data-en-lrev/colbertv2/indices/bert-base-multilingual-cased/enfever_lrev/nway96_anserini+minilm.2bits\"\n",
    "\n",
    "retriever = ColBERTv2Retriever(IDX_NAME, Path(COLBERT_ROOT, \"original_id2pid.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . Obama was a president, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([     0,      3,      5,  16042,    509,     10,  13918,      2, 250001,\n",
      "        250001, 250001, 250001, 250001, 250001, 250001, 250001, 250001, 250001,\n",
      "        250001, 250001, 250001, 250001, 250001, 250001, 250001, 250001, 250001,\n",
      "        250001, 250001, 250001, 250001, 250001])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Barack_Obama',\n",
       "  'Electoral_history_of_Barack_Obama',\n",
       "  'Barack_-LRB-disambiguation-RRB-'],\n",
       " [24.9375, 24.8125, 23.703125])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.retrieve(\"Obama was a president\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fever_predictions(claims_jsonl, predictions_jsonl, retriever: ColBERTv2Retriever, k:int=500):\n",
    "    test_data = read_jsonl(claims_jsonl)\n",
    "    for r in tqdm(test_data[:]):\n",
    "        ids, _ = retriever.retrieve(r[\"claim\"], k=k)\n",
    "        r[\"predicted_pages\"] = ids\n",
    "    write_jsonl(predictions_jsonl, test_data, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [05:46<00:00, 28.89it/s]\n"
     ]
    }
   ],
   "source": [
    "SPLIT = \"paper_test\"\n",
    "\n",
    "# generate_fever_predictions(Path(FEVER_ROOT, f\"{SPLIT}.jsonl\"), \n",
    "#                            Path(FEVER_PREDICTIONS, f\"{SPLIT}/colbertv2/bert-base-uncased/ms-marco+enfever_lrev/triples64_o0_anserini_negatives_cp20k.jsonl\"), retriever, k=500)\n",
    "\n",
    "# generate_fever_predictions(Path(FEVER_ROOT, f\"{SPLIT}.jsonl\"), \n",
    "#                            Path(FEVER_PREDICTIONS, f\"{SPLIT}/colbertv2/bert-base-uncased/msmarco/k500.jsonl\"), retriever, k=500)\n",
    "\n",
    "# generate_fever_predictions(Path(FEVER_ROOT, f\"{SPLIT}.jsonl\"), \n",
    "#                            Path(FEVER_PREDICTIONS, f\"{SPLIT}/colbertv2/bert-base-multilingual-cased/enfever_lrev/triples64_o0_anserini+minilm_cp30k.2bits.jsonl\"), retriever, k=500)\n",
    "\n",
    "# generate_fever_predictions(Path(FEVER_ROOT, f\"{SPLIT}.jsonl\"), \n",
    "#                            Path(FEVER_PREDICTIONS, f\"{SPLIT}/colbertv2/bert-base-multilingual-cased/enfever_lrev/nway96_anserini+minilm.2bits.jsonl\"), retriever, k=500)\n",
    "\n",
    "# generate_fever_predictions(Path(FEVER_ROOT, f\"{SPLIT}.jsonl\"), \n",
    "#                            Path(FEVER_PREDICTIONS, f\"{SPLIT}/colbertv2/bert-base-uncased/enfever_lrev/nway32_anserini+minilm.2bits.jsonl\"), retriever, k=500)\n",
    "\n",
    "generate_fever_predictions(Path(FEVER_ROOT, f\"{SPLIT}.jsonl\"), \n",
    "                           Path(FEVER_PREDICTIONS, f\"{SPLIT}/colbertv2/xlm-roberta-large-squad2/enfever_lrev/nway32_anserini+minilm.2bits.jsonl\"), retriever, k=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid2original_id = {v: k for k, v in original_id2pid.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(pid2original_id[t[1]] for t in trn_triples)\n",
    "c.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hflarge",
   "language": "python",
   "name": "hflarge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
