{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drchajan/devel/python/FC/aic-nlp-utils/aic_nlp_utils/json.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "/mnt/appl/software/PyTorch/2.0.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "No CUDA runtime is found, using CUDA_HOME='/mnt/appl/software/CUDA/11.7.0'\n",
      "/home/drchajan/devel/python/FC/fc_env_hflarge/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/drchajan/devel/python/FC/fc_env_hflarge/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sqlite3\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from dataclasses import dataclass\n",
    "import datetime as dt\n",
    "from itertools import chain\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import string\n",
    "import pandas as pd\n",
    "import unicodedata as ud\n",
    "from time import time\n",
    "from typing import Dict, Type, Callable, List, Union\n",
    "import sys\n",
    "import ujson\n",
    "\n",
    "from aic_nlp_utils.json import read_jsonl, read_json, write_json, write_jsonl\n",
    "from aic_nlp_utils.encoding import nfc\n",
    "from aic_nlp_utils.fever import fever_detokenize\n",
    "from sentence_transformers import CrossEncoder, util\n",
    "import textwrap\n",
    "\n",
    "sys.path.insert(0, '/home/drchajan/devel/python/FC/ColBERTv2') # ignore other ColBERT installations\n",
    "\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert import Indexer, Searcher\n",
    "from colbert.data import Queries, Collection\n",
    "from colbert import Trainer\n",
    "from colbert.utilities.prepare_data import import_qacg_split, import_qacg_split_subsample, generate_original_id2pid_mapping, export_as_anserini_collection, anserini_retrieve_claims, sbert_CE_rerank, generate_triples_by_retrieval, generate_triples_by_retrieval_nway\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_sqlite(in_path, out_jsonl):\n",
    "    recs = []\n",
    "    with sqlite3.connect(in_path, detect_types=sqlite3.PARSE_DECLTYPES) as connection:\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"SELECT id, did, bid, date, keywords, text FROM documents\")\n",
    "\n",
    "        row = 0\n",
    "        nexcluded = 0\n",
    "        for id_, did, bid, date, keywords, text in cursor.fetchall():\n",
    "            rec = {\"id\": id_, \"did\": did, \"bid\": bid, \"date\": date, \"keywords\": keywords, \"text\": text}\n",
    "            recs.append(rec)\n",
    "        write_jsonl(out_jsonl, recs)\n",
    "    \n",
    "\n",
    "data = import_sqlite(\"/mnt/data/ctknews/factcheck/par6/interim/ctk_filtered.db\", \"/mnt/data/ctknews/factcheck/par6/interim/jsonl/ctk_filtered_all_cols.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/ctknews/factcheck/par6/qacg/splits/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# APPROACH = \"full\" # all generated data\n",
    "# APPROACH = \"balanced\" # balanced classes\n",
    "APPROACH = \"balanced_shuf\" # balanced classes, shuffled\n",
    "# APPROACH = \"fever_size\" # subsampled to have exact fever distribution\n",
    "\n",
    "LANG, NER_DIR, ANSERINI_LANG = \"cs\", \"PAV-ner-CNEC\", \"cs\"\n",
    "\n",
    "# DATA_ROOT = f\"/mnt/data/cro/factcheck/v1\"\n",
    "# DATA_CORPUS = Path(DATA_ROOT, \"interim\", \"cro_paragraphs_filtered.jsonl\")\n",
    "# TRN_SIZE, DEV_SIZE, TST_SIZE = 20000, 2000, 2000\n",
    "\n",
    "DATA_ROOT = f\"/mnt/data/ctknews/factcheck/par6\"\n",
    "DATA_CORPUS = Path(DATA_ROOT, \"interim\", \"jsonl\", \"ctk_filtered_all_cols.jsonl\")\n",
    "TRN_SIZE, DEV_SIZE, TST_SIZE = 40000, 4000, 4000\n",
    "\n",
    "# DATA_ROOT = f\"/mnt/data/factcheck/denikn/v1\"\n",
    "# DATA_CORPUS = Path(DATA_ROOT, \"interim\", \"denikn_paragraphs.jsonl\")\n",
    "# TRN_SIZE, DEV_SIZE, TST_SIZE = 20000, 2000, 2000\n",
    "\n",
    "# DATA_ROOT = f\"/mnt/data/newton/parlamentni_listy/factcheck/v1\"\n",
    "# DATA_CORPUS = Path(DATA_ROOT, \"interim\", \"plisty_paragraphs.jsonl\")\n",
    "# TRN_SIZE, DEV_SIZE, TST_SIZE = 20000, 2000, 2000\n",
    "\n",
    "QACG_ROOT = Path(DATA_ROOT, \"qacg\")\n",
    "# WIKI_PREDICTIONS = f\"{WIKI_ROOT}/predictions\"\n",
    "\n",
    "QG_DIR = \"mt5-large_all-cp126k\"\n",
    "QACG_DIR = \"mt5-large_all-cp156k\"\n",
    "\n",
    "SPLIT_DIR = Path(\"splits\", NER_DIR, QG_DIR, QACG_DIR)\n",
    "SPLIT_ROOT = Path(QACG_ROOT, SPLIT_DIR)\n",
    "\n",
    "CLAIM_DIR = Path(\"claim\", NER_DIR, QG_DIR, QACG_DIR)\n",
    "CLAIM_ROOT = Path(QACG_ROOT, CLAIM_DIR)\n",
    "\n",
    "\n",
    "TRAIN_FILES = {\n",
    "    \"s\": Path(CLAIM_ROOT, \"train_support.json\"), \n",
    "    \"r\": Path(CLAIM_ROOT, \"train_refute.json\"),\n",
    "    \"n\": Path(CLAIM_ROOT, \"train_nei.json\") # JUST to generate splits for NLI, nothing else concerning ColBERT!\n",
    "    }\n",
    "\n",
    "DEV_FILES = {\n",
    "    \"s\": Path(CLAIM_ROOT, \"dev_support.json\"), \n",
    "    \"r\": Path(CLAIM_ROOT, \"dev_refute.json\"),\n",
    "    \"n\": Path(CLAIM_ROOT, \"dev_nei.json\")\n",
    "    }\n",
    "\n",
    "TEST_FILES = {\n",
    "    \"s\": Path(CLAIM_ROOT, \"test_support.json\"), \n",
    "    \"r\": Path(CLAIM_ROOT, \"test_refute.json\"),\n",
    "    \"n\": Path(CLAIM_ROOT, \"test_nei.json\")\n",
    "    }\n",
    "\n",
    "COLBERT_ROOT = Path(DATA_ROOT, \"colbertv2/qacg\")\n",
    "LINENO2ID = Path(COLBERT_ROOT, \"paragraphs_lineno2id.json\")\n",
    "\n",
    "\n",
    "QUERIES_ROOT = Path(COLBERT_ROOT, \"queries\", NER_DIR, QG_DIR, QACG_DIR)\n",
    "TRIPLES_ROOT = Path(COLBERT_ROOT, \"triples\", NER_DIR, QG_DIR, QACG_DIR)\n",
    "\n",
    "ANSERINI_ROOT = Path(DATA_ROOT, \"anserini\")\n",
    "ANSERINI_COLLECTION = str(Path(ANSERINI_ROOT, \"collection\"))\n",
    "ANSERINI_INDEX = str(Path(ANSERINI_ROOT, \"index\"))\n",
    "ANSERINI_RETRIEVED = Path(ANSERINI_ROOT, \"retrieved\", NER_DIR, QG_DIR, QACG_DIR)\n",
    "SPLIT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_corpus(corpus_file):\n",
    "    # it already has correct format\n",
    "    raw = read_jsonl(corpus_file, show_progress=True)\n",
    "    for e in raw:\n",
    "        e[\"id\"] = nfc(e[\"id\"])\n",
    "        if \"did\" not in e:\n",
    "            did, bid = e[\"id\"].split(\"_\")\n",
    "            e[\"bid\"] = bid\n",
    "            e[\"did\"] = did\n",
    "        e[\"did\"] = nfc(str(e[\"did\"]))\n",
    "        e[\"text\"] = nfc(e[\"text\"])\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008971452713012695,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af07af62da474143a5b4682caf864713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '20000818F01557_1',\n",
       " 'did': '20000818F01557',\n",
       " 'bid': 1,\n",
       " 'date': '2000-08-18T14:24:00Z',\n",
       " 'keywords': 'EU;Kolumbie;automoto;Volkswagen',\n",
       " 'text': 'BRUSEL 18. srpna (ČTK) - Evropská komise (EK) jako protimonopolní orgán Evropské unie se začala zabývat stížností německé automobilky Volkswagen, která není spokojena s daňovým režimem v jihoamerické Kolumbii. Automobilka je přesvědčena, že Kolumbie svou daňovou politikou diskriminuje zahraniční dovozce malých automobilů.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = import_corpus(DATA_CORPUS)\n",
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents: 2151114 paragraphs: 11754759, paragraphs per document: 5.464498394785213\n",
      "paragraph len: min:1, max:341451, mean:300.6763692900892, median:303.0\n"
     ]
    }
   ],
   "source": [
    "def print_stats(corpus):\n",
    "    did_set = set([r[\"did\"] for r in corpus])\n",
    "    ndoc = len(did_set)\n",
    "    npar = len(corpus)\n",
    "    print(f\"documents: {ndoc} paragraphs: {npar}, paragraphs per document: {npar/ndoc}\")\n",
    "    plens = [len(r[\"text\"]) for r in corpus]\n",
    "    print(f\"paragraph len: min:{np.min(plens)}, max:{np.max(plens)}, mean:{np.mean(plens)}, median:{np.median(plens)}\")\n",
    "\n",
    "print_stats(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_id2pid = generate_original_id2pid_mapping(corpus)\n",
    "lineno2id = {i: r[\"id\"] for i, r in enumerate(corpus)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN this just for the first time\n",
    "write_json(Path(COLBERT_ROOT, \"original_id2pid.json\"), original_id2pid, mkdir=True)\n",
    "write_jsonl(Path(COLBERT_ROOT, \"collection.jsonl\"), corpus, mkdir=True)\n",
    "write_json(LINENO2ID, lineno2id, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/newton/parlamentni_listy/factcheck/v1/colbertv2/qacg/paragraphs_lineno2id.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LINENO2ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading: /mnt/data/newton/parlamentni_listy/factcheck/v1/qacg/claim/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/train_support.json\n",
      "reading: /mnt/data/newton/parlamentni_listy/factcheck/v1/qacg/claim/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/train_refute.json\n",
      "reading: /mnt/data/newton/parlamentni_listy/factcheck/v1/qacg/claim/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/train_nei.json\n",
      "reading: /mnt/data/newton/parlamentni_listy/factcheck/v1/qacg/claim/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/dev_support.json\n",
      "reading: /mnt/data/newton/parlamentni_listy/factcheck/v1/qacg/claim/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/dev_refute.json\n",
      "reading: /mnt/data/newton/parlamentni_listy/factcheck/v1/qacg/claim/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/dev_nei.json\n",
      "reading: /mnt/data/newton/parlamentni_listy/factcheck/v1/qacg/claim/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/test_support.json\n",
      "reading: /mnt/data/newton/parlamentni_listy/factcheck/v1/qacg/claim/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/test_refute.json\n",
      "reading: /mnt/data/newton/parlamentni_listy/factcheck/v1/qacg/claim/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/test_nei.json\n"
     ]
    }
   ],
   "source": [
    "if APPROACH == \"full\":\n",
    "    trn_data = import_qacg_split(TRAIN_FILES)\n",
    "    dev_data = import_qacg_split(DEV_FILES)\n",
    "    tst_data = import_qacg_split(TEST_FILES)\n",
    "elif APPROACH in [\"balanced\", \"balanced_shuf\"]:\n",
    "    trn_data = import_qacg_split_subsample(TRAIN_FILES, subsample=TRN_SIZE, seed=1234)\n",
    "    dev_data = import_qacg_split_subsample(DEV_FILES, subsample=DEV_SIZE, seed=1234)\n",
    "    tst_data = import_qacg_split_subsample(TEST_FILES, subsample=TST_SIZE, seed=1234)\n",
    "    if APPROACH == \"balanced_shuf\":\n",
    "        rng = np.random.RandomState(1234)\n",
    "        rng.shuffle(trn_data)\n",
    "        rng.shuffle(dev_data)\n",
    "        rng.shuffle(tst_data)\n",
    "elif APPROACH == \"fever_size\":\n",
    "    print(\"Created in prepare_data_fever.ipynb\")\n",
    "    trn_data = read_jsonl(Path(SPLIT_ROOT, \"train_fever_size.jsonl\"))\n",
    "    dev_data = read_jsonl(Path(SPLIT_ROOT, \"dev_fever_size.jsonl\"))\n",
    "    tst_data = read_jsonl(Path(SPLIT_ROOT, \"test_fever_size.jsonl\"))\n",
    "else:\n",
    "    assert False, APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original claims: 60000, unique: 57951\n",
      "original claims: 6000, unique: 5904\n",
      "original claims: 6000, unique: 5872\n",
      "original claims: 40000, unique: 38639\n",
      "original claims: 4000, unique: 3916\n",
      "original claims: 4000, unique: 3889\n"
     ]
    }
   ],
   "source": [
    "def unique_claims(data):\n",
    "    claims = set()\n",
    "    new_data = []\n",
    "    for e in data:\n",
    "        if e[\"claim\"] not in claims:\n",
    "            new_data.append(e)\n",
    "            claims.add(e[\"claim\"])\n",
    "    print(f\"original claims: {len(data)}, unique: {len(new_data)}\")\n",
    "    return new_data\n",
    "\n",
    "trn_data_all = unique_claims(trn_data)\n",
    "dev_data_all = unique_claims(dev_data)\n",
    "tst_data_all = unique_claims(tst_data)\n",
    "\n",
    "trn_data = unique_claims([e for e in trn_data if e[\"label\"] != 'n'])\n",
    "dev_data = unique_claims([e for e in dev_data if e[\"label\"] != 'n'])\n",
    "tst_data = unique_claims([e for e in tst_data if e[\"label\"] != 'n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/newton/parlamentni_listy/factcheck/v1/qacg/splits/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_jsonl(Path(SPLIT_ROOT, f\"train_{APPROACH}.jsonl\"), trn_data_all, mkdir=True)\n",
    "write_jsonl(Path(SPLIT_ROOT, f\"dev_{APPROACH}.jsonl\"), dev_data_all, mkdir=True)\n",
    "write_jsonl(Path(SPLIT_ROOT, f\"test_{APPROACH}.jsonl\"), tst_data_all, mkdir=True)\n",
    "write_jsonl(Path(SPLIT_ROOT, f\"train_{APPROACH}_no_nei.jsonl\"), trn_data, mkdir=True)\n",
    "write_jsonl(Path(SPLIT_ROOT, f\"dev_{APPROACH}_no_nei.jsonl\"), dev_data, mkdir=True)\n",
    "write_jsonl(Path(SPLIT_ROOT, f\"test_{APPROACH}_no_nei.jsonl\"), tst_data, mkdir=True)\n",
    "SPLIT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'claim': 'Pirátská strana se snaží dostat seniory na palubu.',\n",
       "  'label': 'r',\n",
       "  'evidence': ['H59A21CY0077_15']},\n",
       " {'claim': 'Cílem útoku je Ukrajina.',\n",
       "  'label': 'r',\n",
       "  'evidence': ['H59A22930039_2']},\n",
       " {'claim': 'Česká Lípa byla úklidem města.',\n",
       "  'label': 's',\n",
       "  'evidence': ['H59A22AH0102_2']})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_data[0], trn_data[1], trn_data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38639/38639 [00:00<00:00, 1230533.42it/s]\n",
      "100%|██████████| 3916/3916 [00:00<00:00, 1488166.57it/s]\n",
      "100%|██████████| 3889/3889 [00:00<00:00, 1669223.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/newton/parlamentni_listy/factcheck/v1/colbertv2/qacg/queries/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def export_queries(data, out_file):\n",
    "    queries = []\n",
    "    for r in tqdm(data):\n",
    "        queries.append({\"query\": r[\"claim\"]})\n",
    "    write_jsonl(out_file, queries, mkdir=True)\n",
    "\n",
    "export_queries(trn_data, Path(QUERIES_ROOT, f\"train_qacg_queries_{APPROACH}.jsonl\"))\n",
    "export_queries(dev_data, Path(QUERIES_ROOT, f\"dev_qacg_queries_{APPROACH}.jsonl\"))\n",
    "export_queries(tst_data, Path(QUERIES_ROOT, f\"test_qacg_queries_{APPROACH}.jsonl\"))\n",
    "QUERIES_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anserini Hard Negatives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Anserini in the first stage to get hard negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_as_anserini_collection(corpus, ANSERINI_COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-11-26 15:36:21,795 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-11-26 15:36:21,797 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-11-26 15:36:21,797 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-11-26 15:36:21,797 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: /mnt/data/ctknews/factcheck/par6/anserini/collection\n",
      "2023-11-26 15:36:21,798 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-11-26 15:36:21,798 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-11-26 15:36:21,798 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 4\n",
      "2023-11-26 15:36:21,798 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: cs\n",
      "2023-11-26 15:36:21,799 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-11-26 15:36:21,799 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-11-26 15:36:21,799 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-11-26 15:36:21,799 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-11-26 15:36:21,799 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-11-26 15:36:21,800 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-11-26 15:36:21,800 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-11-26 15:36:21,800 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-11-26 15:36:21,800 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-11-26 15:36:21,800 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-11-26 15:36:21,801 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: false\n",
      "2023-11-26 15:36:21,801 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: /mnt/data/ctknews/factcheck/par6/anserini/index\n",
      "2023-11-26 15:36:21,805 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-11-26 15:36:21,816 INFO  [main] index.IndexCollection (IndexCollection.java:468) - Using DefaultEnglishAnalyzer\n",
      "2023-11-26 15:36:21,816 INFO  [main] index.IndexCollection (IndexCollection.java:469) - Stemmer: porter\n",
      "2023-11-26 15:36:21,816 INFO  [main] index.IndexCollection (IndexCollection.java:470) - Keep stopwords? false\n",
      "2023-11-26 15:36:21,817 INFO  [main] index.IndexCollection (IndexCollection.java:471) - Stopwords file: null\n",
      "2023-11-26 15:36:21,885 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 4 threads initialized.\n",
      "2023-11-26 15:36:21,885 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in /mnt/data/ctknews/factcheck/par6/anserini/collection\n",
      "2023-11-26 15:36:21,887 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
      "2023-11-26 15:36:21,887 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-11-26 15:37:21,889 INFO  [main] index.IndexCollection (IndexCollection.java:534) - 1,330,000 documents indexed\n",
      "2023-11-26 15:38:21,892 INFO  [main] index.IndexCollection (IndexCollection.java:534) - 2,470,000 documents indexed\n",
      "2023-11-26 15:39:21,893 INFO  [main] index.IndexCollection (IndexCollection.java:534) - 3,600,000 documents indexed\n",
      "2023-11-26 15:40:21,894 INFO  [main] index.IndexCollection (IndexCollection.java:534) - 4,670,000 documents indexed\n",
      "2023-11-26 15:41:21,896 INFO  [main] index.IndexCollection (IndexCollection.java:534) - 5,690,000 documents indexed\n",
      "2023-11-26 15:42:21,897 INFO  [main] index.IndexCollection (IndexCollection.java:534) - 6,770,000 documents indexed\n",
      "2023-11-26 15:43:21,898 INFO  [main] index.IndexCollection (IndexCollection.java:534) - 7,850,000 documents indexed\n",
      "2023-11-26 15:44:21,899 INFO  [main] index.IndexCollection (IndexCollection.java:534) - 8,850,000 documents indexed\n",
      "2023-11-26 15:45:21,901 INFO  [main] index.IndexCollection (IndexCollection.java:534) - 9,640,000 documents indexed\n",
      "2023-11-26 15:46:21,902 INFO  [main] index.IndexCollection (IndexCollection.java:534) - 10,200,000 documents indexed\n",
      "2023-11-26 15:47:21,904 INFO  [main] index.IndexCollection (IndexCollection.java:534) - 11,390,000 documents indexed\n",
      "2023-11-26 15:47:40,814 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection/docs.json: 11754759 docs added.\n",
      "2023-11-26 15:47:58,690 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 11,754,759 documents indexed\n",
      "2023-11-26 15:47:58,690 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-11-26 15:47:58,690 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:       11,754,759\n",
      "2023-11-26 15:47:58,690 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-11-26 15:47:58,691 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-11-26 15:47:58,691 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-11-26 15:47:58,691 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-11-26 15:47:58,721 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 11,754,759 documents indexed in 00:11:36\n"
     ]
    }
   ],
   "source": [
    "!python -m pyserini.index.lucene \\\n",
    "    -collection JsonCollection \\\n",
    "    -generator DefaultLuceneDocumentGenerator \\\n",
    "    -threads 4 \\\n",
    "    -input {ANSERINI_COLLECTION} \\\n",
    "    -language {ANSERINI_LANG} \\\n",
    "    -index {ANSERINI_INDEX} \\\n",
    "    -storePositions -storeDocvectors -storeRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/newton/parlamentni_listy/factcheck/v1/anserini/retrieved/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANSERINI_RETRIEVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3916/3916 [00:37<00:00, 105.19it/s]\n",
      "100%|██████████| 3889/3889 [00:36<00:00, 105.96it/s]\n",
      "100%|██████████| 38639/38639 [06:01<00:00, 107.01it/s]\n"
     ]
    }
   ],
   "source": [
    "anserini_retrieve_claims(ANSERINI_INDEX, dev_data, 128)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, f\"dev_{APPROACH}.jsonl\"), dev_data, mkdir=True)\n",
    "\n",
    "anserini_retrieve_claims(ANSERINI_INDEX, tst_data, 128)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, f\"test_{APPROACH}.jsonl\"), tst_data, mkdir=True)\n",
    "\n",
    "anserini_retrieve_claims(ANSERINI_INDEX, trn_data, 128)\n",
    "write_jsonl(Path(ANSERINI_RETRIEVED, f\"train_{APPROACH}.jsonl\"), trn_data, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = read_jsonl(Path(ANSERINI_RETRIEVED, f\"train_{APPROACH}.jsonl\"))\n",
    "dev_data = read_jsonl(Path(ANSERINI_RETRIEVED, f\"dev_{APPROACH}.jsonl\"))\n",
    "tst_data = read_jsonl(Path(ANSERINI_RETRIEVED, f\"test_{APPROACH}.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/newton/parlamentni_listy/factcheck/v1/anserini/retrieved/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANSERINI_RETRIEVED"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = read_jsonl(Path(ANSERINI_RETRIEVED, f\"train_{APPROACH}.jsonl\"))\n",
    "dev_data = read_jsonl(Path(ANSERINI_RETRIEVED, f\"dev_{APPROACH}.jsonl\"))\n",
    "tst_data = read_jsonl(Path(ANSERINI_RETRIEVED, f\"test_{APPROACH}.jsonl\"))\n",
    "# trn_data = read_jsonl(Path(ANSERINI_RETRIEVED, \"train_anserini+minilm.jsonl\"))\n",
    "# dev_data = read_jsonl(Path(ANSERINI_RETRIEVED, \"dev_anserini+minilm.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAIM: Hnutí ANO bylo historicky prvním hnutí, které vyhrálo.\n",
      "\n",
      "EVIDENCE (H59A21DS0046_4) NOT FOUND 'Niles':\n",
      "Ještě v dubnu mohli gratulace za historicky první pokoření hnutí ANO\n",
      "přijímat Piráti v koalici se Starosty a Nezávislými (STAN). Ale jak se\n",
      "ukázalo i zde, první vyhrání leckdy z kapsy vyhání. PirSTAN vyhnalo z\n",
      "prvního místa, křivka úspěšnosti této koalice se vydala na trajektorii\n",
      "postupného, leč vytrvalého pádu, v jehož průběhu ji dokázalo přeskočit\n",
      "konkurenční koaliční uskupení SPOLU (ODS+TOP 09+KDU-ČSL) a staronový\n",
      "hegemon žebříčku hnutí ANO.\n",
      "\n",
      "RETRIEVED 1 (H59A21GE0032_11) NOT FOUND 'Niles':\n",
      "To je sice pravda, ale když se na to podívám celkově, tak vyhrálo\n",
      "hnutí ANO, vždyť kolik hlasů dostaly ty strany a straničky v\n",
      "jednotlivých koalicích? Kolik dohromady dostaly dvě historicky\n",
      "největší a nejúspěšnější strany ODS a ČSSD, které dřív měly přece\n",
      "tolik hlasů. Ony hrály, řečeno hokejovou hantýrkou, NHL a pak najednou\n",
      "padaly a padaly o soutěže dolů.\n",
      "\n",
      "RETRIEVED 2 (H59A22F50063_9) NOT FOUND 'Niles':\n",
      "O vítězství hnutí ANO hovořil i zářijový průzkum SANEP. Podle něj by\n",
      "hnutí vyhrálo se ziskem 29,9 %, na druhém místě by nyní skončila dosud\n",
      "nejsilnější strana vládní pětikoalice ODS premiéra Petra Fialy, která\n",
      "by získala 13,1 %. Třetí by pak bylo hnutí SPD, které by získalo 12,6\n",
      "%. Čtvrtým politickým subjektem, který by se v září letošního roku\n",
      "dostal do Sněmovny, by byli Piráti se ziskem 7,4 %.\n",
      "\n",
      "RETRIEVED 3 (H59A22EU0093_2) NOT FOUND 'Niles':\n",
      "„Druhé kolo do Senátu není úspěch hnutí ANO,“ přiznal šéf, když\n",
      "reagoval na fakt, že hnutí ANO z 18 střetů s ostatními politickými\n",
      "stranami proměnilo jen tři. „Já samozřejmě jako netradiční politik\n",
      "říkám pravdu,“ prohlásil Babiš na tiskové konferenci, kde konstatoval,\n",
      "že hnutí ANO jednoznačně vyhrálo na Karlovarsku, protože vyhrálo\n",
      "komunální volby ve zdejších okresních městech a ještě získalo\n",
      "senátorku. Konkrétně Věru Procházkovou.\n"
     ]
    }
   ],
   "source": [
    "def show_retrieval(sample, corpus, search, k=3):\n",
    "    claim = sample[\"claim\"]\n",
    "    bid = sample[\"evidence\"][0]\n",
    "    rec = corpus[original_id2pid[bid]]\n",
    "    evidence = rec[\"text\"]\n",
    "    # retrieved = sample[\"retrieved\"]\n",
    "    print(f\"CLAIM: {claim}\")\n",
    "    print()\n",
    "\n",
    "    if search:\n",
    "        found = f\"FOUND '{search}'\" if search in evidence else f\"NOT FOUND '{search}'\"\n",
    "    else:\n",
    "        found = \"\"\n",
    "\n",
    "    print(f\"EVIDENCE ({bid}) {found}:\\n\" + textwrap.fill(evidence))\n",
    "\n",
    "    for i in range(k):\n",
    "        bid = sample[\"retrieved\"][i]\n",
    "        ret = corpus[original_id2pid[bid]][\"text\"]\n",
    "        if search:\n",
    "            found = f\"FOUND '{search}'\" if search in ret else f\"NOT FOUND '{search}'\"\n",
    "        else:\n",
    "            found = \"\"\n",
    "        print(f\"\\nRETRIEVED {i+1} ({bid}) {found}:\\n\" + textwrap.fill(ret))\n",
    "\n",
    "\n",
    "\n",
    "show_retrieval(tst_data[11], corpus, search=\"Niles\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triples by Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1822/38639 [00:00<00:11, 3293.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 53 fixing by random\n",
      "WARNING: not enough retrieved documents 128 => 53 fixing by random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 3581/38639 [00:01<00:09, 3512.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 12 fixing by random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 5198/38639 [00:01<00:09, 3657.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: more than 3 occurences of too few retrieved documents, fixing by random...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38639/38639 [00:08<00:00, 4309.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 38639 triples with 0 failures and 24 random fixes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 2539/3916 [00:00<00:00, 4609.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 0 fixing by random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3916/3916 [00:00<00:00, 4660.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 45 fixing by random\n",
      "generated 3916 triples with 0 failures and 2 random fixes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 6/3889 [00:00<02:11, 29.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 56 fixing by random\n",
      "WARNING: not enough retrieved documents 128 => 55 fixing by random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 1253/3889 [00:00<00:01, 1869.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: not enough retrieved documents 128 => 67 fixing by random\n",
      "WARNING: more than 3 occurences of too few retrieved documents, fixing by random...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3889/3889 [00:01<00:00, 2088.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 3889 triples with 0 failures and 7 random fixes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nway = 128\n",
    "\n",
    "trn_triples = generate_triples_by_retrieval_nway(trn_data, corpus, original_id2pid, nway=nway, use_evidence=True)\n",
    "write_jsonl(Path(TRIPLES_ROOT, f\"trn_triples_nway{nway}_evidence+anserini_{APPROACH}.jsonl\"), trn_triples, mkdir=True)\n",
    "\n",
    "dev_triples = generate_triples_by_retrieval_nway(dev_data, corpus, original_id2pid, nway=nway, use_evidence=True)\n",
    "write_jsonl(Path(TRIPLES_ROOT, f\"dev_triples_nway{nway}_evidence+anserini_{APPROACH}.jsonl\"), dev_triples, mkdir=True)\n",
    "\n",
    "tst_triples = generate_triples_by_retrieval_nway(tst_data, corpus, original_id2pid, nway=nway, use_evidence=True)\n",
    "write_jsonl(Path(TRIPLES_ROOT, f\"tst_triples_nway{nway}_evidence+anserini_{APPROACH}.jsonl\"), tst_triples, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/newton/parlamentni_listy/factcheck/v1/colbertv2/qacg/triples/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIPLES_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_queries_triples_collection(\n",
    "        split_files, # already created combined split\n",
    "        triple_files, # target files for triples (keep same order of splits!)\n",
    "        query_files, # target files for queries (keep same order of splits!)\n",
    "        collection_dir, # combined collection dir\n",
    "        source2triple_files, # triple files for each particular source language\n",
    "        source2collection_files\n",
    "        ):\n",
    "    \n",
    "    #import splits and generate queries (just claims) \n",
    "    splits = []\n",
    "    for split_file, query_file in zip(split_files, query_files):\n",
    "        data = read_jsonl(split_file)\n",
    "        splits.append(data)\n",
    "        recs = [{\"query\": r[\"claim\"]} for r in data]\n",
    "        print(f'writing query file: \"{query_file}\"')\n",
    "        write_jsonl(query_file, recs, mkdir=True)\n",
    "\n",
    "    print(\"loading collections\")\n",
    "    source2collection = {source: read_jsonl(collection_file, show_progress=True) for source, collection_file in source2collection_files.items()}\n",
    "\n",
    "    print(\"loading triples\")\n",
    "    source2triples = {source: [read_jsonl(Path(triple_file_lst[0], triple_file_name)) for triple_file_name in triple_file_lst[1]] for source, triple_file_lst in source2triple_files.items()}\n",
    "\n",
    "    new_collection = []\n",
    "    used_collection_pids = defaultdict(dict) # language to original pid to new pid \n",
    "    new_triples = [[] for _ in range(len(splits))] # dict of list (splits) of lists (triples)\n",
    "\n",
    "    for split_idx, split in enumerate(splits):\n",
    "        # run through the matching split and select all samples with matching `lang`\n",
    "        for s in tqdm(split, desc=f\"split idx: {split_idx}\"):\n",
    "            # print(f\"DEBUG: sample={s}\")\n",
    "            source = s[\"source\"]\n",
    "            # index from the original language split\n",
    "            orig_idx = int(s[\"orig_idx\"])\n",
    "            # print(f\"DEBUG: source={source} split_idx={split_idx} orig_idx={orig_idx}\")\n",
    "            trp = source2triples[source][split_idx][orig_idx]\n",
    "            # print(f\"DEBUG: trp={trp}\")\n",
    "            new_trp_id = len(new_triples[split_idx]) # generate ever increasing triple ids\n",
    "            new_trp = [new_trp_id]\n",
    "            # now translate original pids to new pids\n",
    "            for orig_pid in trp[1:]:\n",
    "                if orig_pid not in used_collection_pids[source]:\n",
    "                    new_pid = len(new_collection) # ever increasing pid given by position in `the new_collection`\n",
    "                    used_collection_pids[source][orig_pid] = new_pid\n",
    "                    col_item = source2collection[source][orig_pid]\n",
    "                    new_collection.append(col_item)\n",
    "                else:\n",
    "                    new_pid = used_collection_pids[source][orig_pid]\n",
    "                new_trp.append(new_pid)\n",
    "            # print(f\"DEBUG: new_trp={new_trp}\")\n",
    "                    \n",
    "            new_triples[split_idx].append(new_trp)\n",
    "    \n",
    "    assert len(new_triples) == len(triple_files)\n",
    "    for nt, triple_file in zip(new_triples, triple_files):\n",
    "        print(f'writing triple file: \"{triple_file}\"')\n",
    "        write_jsonl(triple_file, nt, mkdir=True)\n",
    "\n",
    "    collection_file = Path(collection_dir, \"collection.jsonl\")\n",
    "    print(f'writing collection file: \"{collection_file}\"')\n",
    "    write_jsonl(collection_file, new_collection, mkdir=True, show_progress=True)\n",
    "\n",
    "    # Most likely makes no sense as the bids from different Wikipedia may overlap \n",
    "    # original_id2pid = {r[\"id\"]: i for i, r in enumerate(new_collection)}\n",
    "    # original_id2pid_file = Path(collection_dir, \"original_id2pid.json\")\n",
    "    # print(f'writing original_id2pid file: \"{original_id2pid_file}\"')\n",
    "    # write_json(original_id2pid_file, original_id2pid, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing query file: \"/mnt/data/factcheck/qacg/news_sum/colbertv2/qacg/queries/dev_qacg_queries_balanced_shuf.jsonl\"\n",
      "writing query file: \"/mnt/data/factcheck/qacg/news_sum/colbertv2/qacg/queries/test_qacg_queries_balanced_shuf.jsonl\"\n",
      "writing query file: \"/mnt/data/factcheck/qacg/news_sum/colbertv2/qacg/queries/train_qacg_queries_balanced_shuf.jsonl\"\n",
      "loading collections\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009174823760986328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a11a0623224fa488d78090d9b9d4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011357545852661133,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6344f0156bb4611a07233ecba9920d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009431600570678711,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b70e3bf889b44908a75ae5954ac8455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008755922317504883,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eaf379797984328ae6b8d79ff24213f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading triples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "split idx: 0: 100%|██████████| 19644/19644 [00:01<00:00, 11312.34it/s]\n",
      "split idx: 1: 100%|██████████| 19657/19657 [00:01<00:00, 11925.49it/s]\n",
      "split idx: 2: 100%|██████████| 194972/194972 [00:14<00:00, 13077.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing triple file: \"/mnt/data/factcheck/qacg/news_sum/colbertv2/qacg/triples/dev_triples_nway128_evidence+anserini_balanced_shuf.jsonl\"\n",
      "writing triple file: \"/mnt/data/factcheck/qacg/news_sum/colbertv2/qacg/triples/tst_triples_nway128_evidence+anserini_balanced_shuf.jsonl\"\n",
      "writing triple file: \"/mnt/data/factcheck/qacg/news_sum/colbertv2/qacg/triples/trn_triples_nway128_evidence+anserini_balanced_shuf.jsonl\"\n",
      "writing collection file: \"/mnt/data/factcheck/qacg/news_sum/colbertv2/qacg/collection.jsonl\"\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00891876220703125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 7677707,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdb5f392f014cce9155f211b71b40ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7677707 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "APPROACH=\"balanced_shuf\"\n",
    "QACG_ROOT=f\"/mnt/data/factcheck/qacg/news_sum/qacg\"\n",
    "COLBERT_ROOT=f\"/mnt/data/factcheck/qacg/news_sum/colbertv2/qacg\"\n",
    "COLBERT_ROOT_CRO=f\"/mnt/data/cro/factcheck/v1/colbertv2/qacg\"\n",
    "COLBERT_ROOT_CTK=f\"/mnt/data/ctknews/factcheck/par6/colbertv2/qacg\"\n",
    "COLBERT_ROOT_DENIKN=f\"/mnt/data/factcheck/denikn/v1/colbertv2/qacg\"\n",
    "COLBERT_ROOT_PLISTY=f\"/mnt/data/newton/parlamentni_listy/factcheck/v1/colbertv2/qacg\"\n",
    "MODELS_CS=f\"PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k\"\n",
    "\n",
    "TRIPLE_SPLITS = [\n",
    "    f\"dev_triples_nway128_evidence+anserini_{APPROACH}.jsonl\",\n",
    "    f\"tst_triples_nway128_evidence+anserini_{APPROACH}.jsonl\",\n",
    "    f\"trn_triples_nway128_evidence+anserini_{APPROACH}.jsonl\",\n",
    "    ]\n",
    "\n",
    "st = combine_queries_triples_collection(\n",
    "    split_files=[\n",
    "        f\"{QACG_ROOT}/splits/dev_{APPROACH}_no_nei.jsonl\",\n",
    "        f\"{QACG_ROOT}/splits/test_{APPROACH}_no_nei.jsonl\",\n",
    "        f\"{QACG_ROOT}/splits/train_{APPROACH}_no_nei.jsonl\",\n",
    "    ],\n",
    "    query_files=[\n",
    "        f\"{COLBERT_ROOT}/queries/dev_qacg_queries_{APPROACH}.jsonl\",\n",
    "        f\"{COLBERT_ROOT}/queries/test_qacg_queries_{APPROACH}.jsonl\",\n",
    "        f\"{COLBERT_ROOT}/queries/train_qacg_queries_{APPROACH}.jsonl\",\n",
    "    ],\n",
    "    triple_files=[f\"{COLBERT_ROOT}/triples/{ts}\" for ts in TRIPLE_SPLITS],\n",
    "    collection_dir=COLBERT_ROOT,\n",
    "    source2triple_files = {\n",
    "        \"cro\": (f\"{COLBERT_ROOT_CRO}/triples/{MODELS_CS}\", TRIPLE_SPLITS),\n",
    "        \"ctk\": (f\"{COLBERT_ROOT_CTK}/triples/{MODELS_CS}\", TRIPLE_SPLITS),\n",
    "        \"denikn\": (f\"{COLBERT_ROOT_DENIKN}/triples/{MODELS_CS}\", TRIPLE_SPLITS),\n",
    "        \"plisty\": (f\"{COLBERT_ROOT_PLISTY}/triples/{MODELS_CS}\", TRIPLE_SPLITS),\n",
    "    },\n",
    "    source2collection_files = {\n",
    "        \"cro\": f\"{COLBERT_ROOT_CRO}/collection.jsonl\",\n",
    "        \"ctk\": f\"{COLBERT_ROOT_CTK}/collection.jsonl\",\n",
    "        \"denikn\": f\"{COLBERT_ROOT_DENIKN}/collection.jsonl\",\n",
    "        \"plisty\": f\"{COLBERT_ROOT_PLISTY}/collection.jsonl\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/data/factcheck/qacg/news_sum/qacg'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QACG_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hflarge",
   "language": "python",
   "name": "hflarge"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
